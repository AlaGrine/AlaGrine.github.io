<!DOCTYPE html>
<html lang="en">

<head>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
    integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" />

  <!-- Math formulas -->
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>


  <style type="text/css">
    * {
      scroll-behavior: smooth;
    }

    .scrollToTopBtn {
      position: fixed;
      top: 10%;
      left: 50%;
      transform: translate(-50%, -50%);
      padding: 5px 10px;
      background-color: #27ae60;
      color: #fff;
      border-radius: 25px;
      border-radius: 30px solid #fff;
      cursor: pointer;
      transition: all 0.5s ease 0s;
      /* keep it at the top of everything else */
      z-index: 100;
      /* hide with opacity */
      opacity: 0;

    }

    .showBtn {
      opacity: 1;
    }

    /* Rounded border */
    hr.rounded {
      margin-top: 4%;
      margin-bottom: 3%;
      border-top: 5px solid #bbb;
      border-radius: 3px;
    }

    figure {
      /* display: block; */
      margin-left: auto;
      margin-right: auto;
      text-align: center;
    }

    figcaption {
      background-color: black;
      color: white;
      font-style: italic;
      padding: 2px;
      text-align: center;
    }

    .th1,
    .td1 {
      border: 1px solid black;
      border-radius: 10px;
      padding: 5px 25px;
    }
  </style>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <title>Forecasting Bitcoin closing price series | Ala Eddine GRINE</title>
  <meta name="viewport" content="width=device-width,minimum-scale=1">
  <meta name="description" content="Time series forecasting">
  <meta name="generator" content="Hugo 0.118.2">




  <meta name="robots" content="noindex, nofollow">



  <link rel="stylesheet" href="https://alagrine.github.io/ananke/css/main.min.css">







  <link rel="shortcut icon" href="https://alagrine.github.io/images/portfolio.png" type="image/x-icon" />






  <meta property="og:title" content="Forecasting Bitcoin closing price series" />
  <meta property="og:description" content="Time series forecasting" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="https://alagrine.github.io/post/p6-bitcoin_price_forecast/" />
  <meta property="article:section" content="post" />
  <meta property="article:published_time" content="2023-10-20T11:14:48-04:00" />
  <meta property="article:modified_time" content="2023-10-20T11:14:48-04:00" />
  <meta itemprop="name" content="Forecasting Bitcoin closing price series">
  <meta itemprop="description" content="Time series forecasting">
  <meta itemprop="datePublished" content="2023-10-20T11:14:48-04:00" />
  <meta itemprop="dateModified" content="2023-10-20T11:14:48-04:00" />
  <meta itemprop="wordCount" content="2943">
  <meta itemprop="keywords" content="" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Forecasting Bitcoin closing price series" />
  <meta name="twitter:description" content="Time series forecasting" />


</head>

<body class="ma0 avenir bg-near-white">






  <header class="cover bg-top" style="background-image: url('https://alagrine.github.io/images/P6/BTC_logo2.jpg');">
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
        <div class="flex-l justify-between items-center center">
          <a href="https://alagrine.github.io/" class="f3 fw2 hover-white no-underline white-90 dib">

            Ala Eddine GRINE

          </a>
          <div class="flex-l items-center">



            <ul class="pl0 mr3">

              <li class="list f5 f4-ns fw4 dib pr3">
                <a class="hover-white no-underline white-90" href="https://alagrine.github.io/about/"
                  title="About page">
                  About
                </a>
              </li>

              <li class="list f5 f4-ns fw4 dib pr3">
                <a class="hover-white no-underline white-90" href="https://alagrine.github.io/post/"
                  title="Projects page">
                  Projects
                </a>
              </li>

            </ul>


            <div class="ananke-socials">


              <a href="https://github.com/AlaGrine" target="_blank" rel="noopener"
                class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
                title="GitHub link" aria-label="follow on GitHub——Opens in a new window">

                <span class="icon"><svg style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512"
                    xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z" />
                  </svg>
                </span>

                <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                    viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                    xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                      style="fill-rule:evenodd;clip-rule:evenodd;" />
                  </svg>
                </span></a>


              <a href="https://www.linkedin.com/in/ala-eddine-grine" target="_blank" rel="noopener"
                class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
                title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window">

                <span class="icon"><svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"
                    xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z"
                      style="fill-rule:evenodd;clip-rule:evenodd;" />
                  </svg>
                </span>

                <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                    viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                    xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                      style="fill-rule:evenodd;clip-rule:evenodd;" />
                  </svg>
                </span></a>


              <a href="https://medium.com/@alaeddine.grine" target="_blank" rel="noopener"
                class="medium ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
                title="Medium link" aria-label="follow on Medium——Opens in a new window">

                <span class="icon"><svg style="enable-background:new 0 0 170 170;" version="1.1" viewBox="0 0 170 170"
                    xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M46.5340803,65.2157554 C46.6968378,63.6076572 46.0836,62.018231 44.8828198,60.93592 L32.6512605,46.2010582 L32.6512605,44 L70.6302521,44 L99.9859944,108.380952 L125.794585,44 L162,44 L162,46.2010582 L151.542017,56.2281011 C150.640424,56.9153477 150.193188,58.0448862 150.380019,59.1628454 L150.380019,132.837155 C150.193188,133.955114 150.640424,135.084652 151.542017,135.771899 L161.755369,145.798942 L161.755369,148 L110.38282,148 L110.38282,145.798942 L120.963119,135.527337 C122.002801,134.487948 122.002801,134.182246 122.002801,132.592593 L122.002801,73.0417402 L92.585901,147.755438 L88.6106443,147.755438 L54.3622782,73.0417402 L54.3622782,123.115814 C54.0767278,125.221069 54.7759199,127.3406 56.2581699,128.863022 L70.0186741,145.55438 L70.0186741,147.755438 L31,147.755438 L31,145.55438 L44.7605042,128.863022 C46.2319621,127.338076 46.8903838,125.204485 46.5340803,123.115814 L46.5340803,65.2157554 Z"
                      style="fill-rule:evenodd;clip-rule:evenodd;" />
                  </svg>
                </span>

                <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                    viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                    xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                      style="fill-rule:evenodd;clip-rule:evenodd;" />
                  </svg>
                </span></a>

            </div>

          </div>
        </div>
      </nav>

      <div class="tc-l pv6 ph3 ph4-ns">

        <div class="f2 f1-l fw2 white-90 mb0 lh-title">Forecasting Bitcoin closing price series</div>

        <div class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
          Time series forecasting
        </div>


      </div>
    </div>
  </header>



  <main class="pb7" role="main">


    <article class="flex-l flex-wrap justify-between mw8 center ph3">
      <header class="mt4 w-100">
        <aside class="instapaper_ignoref b helvetica tracked">

          PROJECTS
        </aside>











        <div id="sharing" class="mt3 ananke-socials">


          <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://alagrine.github.io/post/p6-bitcoin_price_forecast/&amp;title=Forecasting%20Bitcoin%20closing%20price%20series"
            class="ananke-social-link linkedin no-underline" aria-label="share on LinkedIn">

            <span class="icon"> <svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"
                xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span>

          </a>

        </div>


        <h1 class="f1 athelas mt3 mb1">Forecasting Bitcoin closing price series</h1>



        <time class="f6 mv4 dib tracked" datetime="2023-10-20T11:14:48-04:00">October 20, 2023</time>




      </header>
      <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-100">

        <h1 id="project-overview">Project Overview:</h1>
        <p>Time series forecasting is one of the most widely used data science techniques in business and finance to
          make future strategic decisions.</p>
        <p>In this project, we will forecast daily closing price series of <a
            href="https://coinmarketcap.com/currencies/bitcoin/">Bitcoin</a>, a peer-to-peer online currency, using
          historical data.</p>
        <p>We will follow different approaches to time series forecasting, implementing both statistical techniques and
          machine learning algorithms.</p>
        <p>We will start with classical approaches to time series forecasting, namely moving averages, exponential
          smoothing methods and the
          ARIMA model.</p>
        <p>We will then use Tensorflow to build a variety of artificial neural networks (including linear, CNN and RNN
          models) with multivariate time series.</p>

        <div class="mb-4">
          <hr class="rounded">
        </div>

        <h1 id="get_data">Get data</h1>
        <p>We can request the historical Bitcoin price via the <a href="https://pypi.org/project/yfinance/">yfinance
            API</a>, which provides a pythonic way to download it, as follows:</p>

        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/6cd545b205edea08784df4e1f402cb96.js"></script>
        <p>As the upward trend is almost absent before 2018, we will exclude earlier dates.</p>


        <div class="mb-4">
          <hr class="rounded">
        </div>

        <h1 id="time-series-analysis">Time Series analysis</h1>
        <p>Here is the evolution of the Bitcoin closing price over time:</p>
        <figure><img src="https://alagrine.github.io/images/P6/1-price.png" />
        </figure>

        <p>The above graph shows no clear trend or seasonality.</p>
        <p>Since 2018, the trend seems to be changing over time. There is a spectacular increase from October 2020 to
          November 2021
          (from 11K to 65K), before a decrease is observed until June 2022. A new increase is observed in 2023.</p>
        <p>We will consider whether a trended method (namely the Holt model) would be better for this series later on.
        </p>
        <p>As our series is a non-seasonal data, we will restrict our attention to non-seasonal ARIMA models.</p>

        <div class="mb-4">
          <hr class="rounded">
        </div>

        <h1 id="train-test-split">Train test split</h1>
        <p>Before we delve into the various methods we will be using to forecast the BTC closing price, let&rsquo;s
          first split our data set into <strong>sequential</strong> train and test subsets.</p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/2ce75d725bde1d96e4abedaed82b21c8.js"></script>

        <p>Note that the test subset will is used to evaluate all of our models. So the comparison will be fair.</p>
        <p>The full training dataset is only used to train our deep learning models with Tensorflow.</p>
        <p>For the other methods (Naive, Exponential Smoothing&hellip;), a window of past data is needed to predict the
          price at time
          \(t\). The size of the window depends on the model. For instance, the
          Naive method requires only the price at time \(t-1\).</p>
        <p>More details on window specifications are given in the next sections.</p>

        <div class="mb-4">
          <hr class="rounded">
        </div>

        <h1 id="baseline--naive-forecasting">Baseline : Naive Forecasting</h1>
        <p>Naive Forecasting is a simple model. It could provide a good baseline for future models.</p>
        <p>It uses the previous timestep value (last day in our case) to predict the next one. The formula can be
          written as follows:</p>
        <p>$$\hat{y}_{t} = y_{t-1}$$</p>
        <figure><img src="https://alagrine.github.io/images/P6/14-naive_forecast.png" />
        </figure>

        <p>If we zoom in, we can see that the Naive forecast lags one step behind the time series.</p>
        <p>Each prediction in the the Naive forecast is off by an average of \($378.9\), which is \( 1.6\% \) of the
          mean
          Bitcoin
          price.</p>
        <p>Our baseline, which we will try to beat with other models, is an <strong>MAE</strong> of \($378.9\).</p>
        <p>Let&rsquo;s start with moving averages.</p>

        <div class="mb-4">
          <hr class="rounded">
        </div>

        <h1 id="simple-moving-average-sma">Simple Moving average (SMA)</h1>
        <p>To estimate the trend of our series, we can use the <a
            href="https://en.wikipedia.org/wiki/Moving_average#Simple_moving_average">Simple
            Moving average</a> algorithm, which takes the average of our time series over either a centered or a
          trailing window as follows:</p>
        <ol>
          <li><strong>Centred window:</strong></li>


          <p>$$ \hat{y}_t = {1 \over (2K+1)} \displaystyle\sum\limits_{i=-K}^{K} {y}_{t+i} $$</p>
          <p>There are no values for either the first or the last K timesteps, because we do not have K observations on
            either side.</p>
        </ol>
        <ol start="2">
          <li><strong>Trailing window:</strong></li>

          <p> $$ \hat{y}_t = {1 \over K} \displaystyle\sum\limits_{i=1}^{K} {y}_{t-i} $$ </p>
          <p>There are no values for the first K timesteps.</p>
        </ol>
        <p> It should be noted that the <strong>K-MA</strong> model assigns the same weight <strong>\(1 \over
            K\)</strong>
          to all observations for either the centred or the trailing window, where \(K\) is the size of the window.
        </p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/04f71ff78f10896494bfbac86f896bab.js"></script>

        <p>As can be seen from the graph below, the larger the window size, the smoother the curve and the less
          sensitive the moving average is to short-term randomness.</p>
        <figure><img src="https://alagrine.github.io/images/P6/15-SMA.png" />
        </figure>

        <p>When using trailing windows, the Naive model (ie. window size =1) yields the best MAE.</p>
        <figure><img src="https://alagrine.github.io/images/P6/16-SMA_windows.png" />
        </figure>

        <blockquote>
          <p>Moving averages with a centred window are more accurate in estimating the trend of our series than moving
            averages with a trailing window.<br>
            <em>For example, a 3-MA gives an MAE of \(190.1\) with a centred window versus \(418.9\) with a trailing
              window.</em>
          </p>
          <p>However, we can&rsquo;t use centred windows to predict present values because we don&rsquo;t know the
            future values (the next <strong>k</strong> values, to be precise).</p>
        </blockquote>
        <figure><img src="https://alagrine.github.io/images/P6/17_SMA_naive_forecats.png" />
        </figure>

        <p>The graph above shows a 14-step-ahead (2 weeks) forecast made by the Naive and 3-MA models.</p>
        <p>We can see that the forecasts are almost flat. We need them to trend like our bitcoin price series.</p>
        <p>To do this we will use exponential smoothing methods (SES and Holt&rsquo;s linear trend method).</p>

        <div class="mb-4">
          <hr class="rounded">
        </div>

        <h1 id="exponontiel-smoothing-ses-and-holt">Exponontiel Smoothing (SES and Holt)</h1>
        <p>In the previous section, we leveraged the simple Moving Average, which assigns equal weight to old and recent
          observations.</p>
        <p><a href="https://www.statsmodels.org/dev/examples/notebooks/generated/exponential_smoothing.html">Exponential
            smoothing methods</a> give a different weight \(w_i\) to each observation \(y_{t-i}\), where \(w_i\)
          decreases
          exponentially as \(i\) increases. That is, more recent values are more important than older ones.</p>
        <ol>
          <li>
            <p>The <strong>Simple Exponential Smoothing</strong> (SES) assigns the following weights: </p>
            <p>$$ \hat{y}_{t} = {\displaystyle\sum\limits_{i=0}^{T} (1-\alpha)^{i} y_{t-i} } $$</p>
            <p>where <strong>T</strong> is the size of our time series and \(0&lt;=\alpha&lt;=1\) is the smoothing
              factor.
            </p>
            <blockquote>
              <p>The <em>SES</em> is suitable for time series with no clear trend or seasonal pattern, such as our
                bitcoin
                price series.</p>
            </blockquote>

          </li>
          <li>
            <p>The <strong>Holt&rsquo;s linear trend method</strong> model extends the SES and applies exponential
              smoothing to both:</p>
            <ul>
              <li><strong>Level:</strong> the weighted average of the obseravations plus the one-step forecast;</li>
              <li><strong>Trend:</strong> the weighted average of the estimated trend (slope) of the series.
              </li>
            </ul>
            <p>Mathematically, this can be written as follows:</p>
            <table class="center">
              <thead>
                <tr>
                  <th></th>
                  <th class="text-center">Holt method</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td class="td1">h-step Forecast Equation</td>
                  <td class="td1">$$ \hat{y}_{t+h} = l_t + h b_t$$</td>
                </tr>
                <tr>
                  <td class="td1">Level Equation</td>
                  <td class="td1">$$ l_t = \alpha y_t + (1-\alpha)(l_{t-1}+b_{t-1}) $$</td>
                </tr>
                <tr>
                  <td class="td1">Trend Equation</td>
                  <td class="td1">$$ b_t = \beta (l_t-l_{t-1}) + (1-\beta)b_{t-1}$$</td>
                </tr>
              </tbody>
            </table>
            <br>
            <p>where \(0&lt;=\alpha&lt;=1\) and \(0&lt;=\beta&lt;=1\) are the smoothing level and the smoothing trend,
              respectively.</p>
            <blockquote>
              <p>The <em>h-step</em> forecasts are a linear function of the horizon \(h\). They
                will increase or decrease indefinitely into the future.</p>
            </blockquote>

            <br>
            <p>To <strong>dampen</strong> the trend to a constant (ie. flat line) some time in the future, a damping
              parameter \(\phi\) could be used as follows:</p>
            <table class="center">
              <thead>
                <tr>
                  <th></th>
                  <th class="text-center">Holt damped method</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td class="td1">h-step Forecast Equation</td>
                  <td class="td1">$$ \hat{y}_{t+h} = l_t + \displaystyle\sum\limits_{i=1}^{h} \phi^{i} b_t$$</td>
                </tr>
                <tr>
                  <td class="td1">Level Equation</td>
                  <td class="td1">$$ l_t = \alpha y_t + (1-\alpha)(l_{t-1}+ \phi b_{t-1}) $$</td>
                </tr>
                <tr>
                  <td class="td1">Trend Equation</td>
                  <td class="td1">$$ b_t = \beta (l_t-l_{t-1}) + (1-\beta)\phi b_{t-1}$$</td>
                </tr>
              </tbody>
            </table>
            <br>
            <p>The Holt&rsquo;s linear method is obtained when \(\phi=1\). </p>
            <p>In practice, we set \(\phi\) betwenn \(0.8\) and \(0.98\).</p>
          </li>
        </ol>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/1ae1b9a8f332d58ecff653fd68b5dc49.js"></script>

        <p>The snippet above shows how to build SES and Holt’s <em>additive</em> and <em>damped</em>
          methods for various window sizes (last month, last year&hellip;) using
          <a
            href="https://www.statsmodels.org/dev/examples/notebooks/generated/exponential_smoothing.html">statsmodels.tsa.holtwinters</a>
          package.
        </p>
        <p>To evaluate these models using the test dataset (y_test):
        <ul>
          <li>A one-step prediction is made.</li>
          <li>For each prediction y_pred[k], we add y_test[k], the new observation, to the list of previous values. We
            then
            retrain the model.</li>
        </ul>
        </p>

        <p>Note that all of the models parameters (including the smoothing level and smoothing trend) are optimized by
          statsmodels (by minimising the <em>SSE</em> or the sum of the squared errors).</p>
        <figure><img src="https://alagrine.github.io/images/P6/18-ExpSmoothing.png" />
        </figure>

        <p>The graph above shows the MAE per window size. For a window size greater than one year, the SES and Holt
          damped methods slightly outperformed the naive
          model, with an MAE of \(377\).</p>
        <p>Comparable MAEs to the damped Holt are obtained with the SES, with a faster execution time, making it the
          best model so far.</p>
        <figure><img src="https://alagrine.github.io/images/P6/19-ExpSmoothing_forecasts.png" />
        </figure>

        <p>As we can see from the graph, the SES does not detect any trends or seasonal patterns. The trend of the
          14-step forecasts is flat.</p>
        <p>With Halt&rsquo;s method, the forecasts are no longer flat but trending: they show a
          <strong>constant</strong> increasing trend into the future.
        </p>

        <div class="mb-4">
          <hr class="rounded">
        </div>

        <h1 id="forecasting-with-arima-using-pmdarima">Forecasting with ARIMA using pmdarima</h1>
        <p>In this section, we will be using the <a
            href="https://alkaline-ml.com/pmdarima/tips_and_tricks.html">ARIMA</a> model, which provides a different
          approach to time series forecasting.</p>
        <p>While exponential smoothing algorithms aim to describe the trend and seasonality of the series, ARIMA models
          exploit the autocorrelations in the data.</p>
        <p>Before we delve into ARIMA models using the <a href="http://alkaline-ml.com/pmdarima/">pmdarima</a> package,
          let&rsquo;s take a look at the autocorrelation function (ACF).</p>
        <br>
        <h2 id="acf-autocorrelation-function">ACF (Autocorrelation Function)</h2>
        <p>Autocorrelation \(r_{k}\) measures the linear relationship between \(y_{t}\) and \(y_{t-k}\), the lagged
          values, as
          follows:</p>
        <p>$$ r_k = {\displaystyle\sum\limits_{t=k+1}^{T} (y_t-\bar y) (y_{t-k}-\bar y) \over
          \displaystyle\sum\limits_{t=1}^{T} (y_t-\bar y)^2} $$</p>
        <p>where <strong>T</strong> is is the length of the time series and \(\bar y\) is its mean.</p>
        <p>
          The autocorrelation function (ACF) consists of autocorrelation coefficients.
          It can be calculated using the <a
            href="https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html">ACF()</a> function
          from <em>statsmodels.tsa.stattools</em>.</p>
        <p>To plot the autocorrelation coefficients, we can use the <a
            href="https://www.statsmodels.org/stable/generated/statsmodels.graphics.tsaplots.plot_acf.html">plot_acf</a>
          function from <em>statsmodels</em> or <a
            href="https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html">autocorrelation_plot</a>
          from <em>Pandas</em>. However, for better interactivity, we will create our own
          <code>plot_xACF_plotly</code> function using <a href="https://plotly.com/python/">Plotly</a> as follows:
        </p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/2e943d9aef8dc13ebf441d10b74e80ea.js"></script>
        <p>Note that in addition to \(r_k\) coefficients, a confidence interval of a weight noise is displayed. </p>
        <p>$$ CI = \pm { 1.96 \over \sqrt{T}} $$ </p>
        <p>Large peaks outside this confidence interval indicate that the series is not stationary.</p>


        <figure><img src="https://alagrine.github.io/images/P6/2-ACF.png" />
        </figure>
        <p>The ACF plot of the BTC price series shows a strong autocorrelation between the current price and the closest
          previous observations,
          and a slow linear decrease from there.</p>

        <br>
        <h2 id="differencing-d">Differencing (d)</h2>
        <p>To address the non-stationarity of our series, we will take a first difference as follows:
          <script type="application/javascript"
            src="https://gist.github.com/AlaGrine/c6044537d736e695bf6c44a31319ab82.js"></script>
        </p>
        <p>Auto-correlation plots are an easy way to determine whether our time series is sufficiently stationary for
          modelling.</p>
        <figure><img src="https://alagrine.github.io/images/P6/3-ACF-diff1.png" />
        </figure>

        <p>The autocorrelation plot of the <em>differences series</em> appears relatively stationary. It shows no
          significant relationship between the lagged observations (all correlations are small and close to zero). </p>


        <br>
        <h2 id="arima">ARIMA</h2>
        <p>ARIMA is an acronym for AutoRegressive Integrated Moving Average. <em>Integrated</em> is the differencing we
          saw in the previous section. The ARIMA(p,d,q) model includes both lagged values of \(y_t\) (AR part) and
          lagged errors \(\epsilon_{t}\) (MA part) and can be written as follows:</p>
        <p>$$ \acute{y}_t = c + {\displaystyle\sum\limits_{i=1}^{p} \phi_{i} \acute{y}_{t-i} +
          \displaystyle\sum\limits_{j=1}^{q} \theta_{j} \epsilon_{t-j} + \epsilon_{t} }$$</p>
        <p>where:</p>
        <ul>
          <li><strong>\(\acute{y}_t\)</strong> is the differenced series (d-order difference, d&gt;=1);</li>
          <li><strong>p</strong> = The order of the autoregressive (AR);</li>
          <li><strong>d</strong> = The degree of the differencing used to obtain a stationary series;</li>
          <li><strong>q</strong> = The order of the moving average (MA) model.</li>
        </ul>
        <p>The parameters <strong>p</strong> and <strong>q</strong> can be iteratively searched-for with the
          <a href="https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html">auto_arima</a>
          function.
        </p>
        <p>The differencing term <strong>d</strong> could be searched for either with the ACF plot or, more
          quantitatively, with tests of stationarity (such as <em>ADF</em> or <em>KPSS</em>).</p>
        <br>

        <h2 id="tests-of-stationarity">Tests of stationarity</h2>
        <p>To estimate the number of differences, we can run a test of stationarity as shown in the snippet below:
        </p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/12c85cc32ba8a359ccae5df753675069.js"></script>

        <p>The tests confirm our conclusion from the ACF plot: one difference (i.e. d = 1) is sufficient to make our
          series stationary.</p>
        <br>
        <h2 id="use-acf-and-pacf-plots-to-determine-appropriate-values-for-p-and-q">Use ACF and PACF plots to determine
          the paremeters (p and q)</h2>
        <p>Partial autocorrelations (<em>PACF</em>) measure the relationship between \(y_{t}\) and \(y_{t-k}\) after
          removing the
          effects of lags 1,2,3,..k.</p>
        </p>
        <p>ACF and PACF can help to estimate the parameters (p and q) of the ARIMA model when significant peaks are
          observed.</p>
        <figure><img src="https://alagrine.github.io/images/P6/4-PACF-diff1.png" />
        </figure>

        <p>Since there is no significant spike in the ACF and PACF plots of the differenced
          data, it is not possible to determine (simply from these plots) whether the data are from an ARIMA(p,d,0) or
          an ARIMA(0,d,q).</p>
        <p>In the next section, we will use the <a
            href="https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html">auto_arima</a>
          process from pmdarima to automatically identify
          the most optimal parameters for an ARIMA(p,d,q) model.</p>
        <br>
        <h2 id="auto_arima">Auto_arima</h2>
        <p>The <a
            href="https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html">auto_arima</a>
          function performed a full or a stepwise search through various combinations of the \(p\), \(d\), and \(q\)
          parameters of ARIMA and selects the parameters that minimize the <strong>AIC</strong> (Akaike&rsquo;s
          Information Criterion) value.</p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/e9d4abc3833776a5239061f1f9b8709d.js"></script>

        <p>The following summary displays the AIC values and estimated model parameters for each candidate model that
          was evaluated.</p>
        <figure><img src="https://alagrine.github.io/images/P6/21-ARIMA_summary.png" /></figure>
        <p>
          Note that the automated stepwise selection has identified an <strong>ARIMA(0,1,0)</strong> model, which is
          equivalent to a <em>random walk</em>.<br>
          We will use the <strong>ARIMA(2,1,2)</strong> identified by the full search.
        </p>
        <br>
        <h2 id="residual-diagnostics">Residual diagnostics</h2>
        <p>It is important to check that the residuals are uncorrelated, so that there is no information left in them,
          and that they have a mean of zero, so that the forecasts are not biased.</p>
        <pre tabindex="0"
          style="overflow: hidden;"><code >ARIMA_residuals = ARIMA_model<span style="color:cyan;">.resid()</span></code></pre>
        <p>
        <figure><img src="https://alagrine.github.io/images/P6/5_residuals_ARIMA.png" />
        </figure>

        <figure><img src="https://alagrine.github.io/images/P6/6_residuals_hist_ARIMA.png" />
        </figure>

        <figure><img src="https://alagrine.github.io/images/P6/7_residuals_ACF_ARIMA.png" />
        </figure>
        </p>
        <p>The ACF plot of the residuals from the ARIMA(2,1,2) model shows that the residuals are uncorrelated.</p>
        <p>Since the mean is 9.2 (not zero), then the forecasts are biased. To handle this, we can add 9.2 to all
          the forecasts and the bias problem is solved.</p>
        <br>
        <h2 id="predictions-with-arima-model">Predictions with ARIMA model</h2>
        <p>To make a one-step-ahead forecast, and as explained <a
            href="https://alkaline-ml.com/pmdarima/refreshing.html">here</a>, we can update our ARIMA model only with
          new observations (in our case, the prices from the y_test dataset), so that future forecasts take into account
          the
          latest observations.</p>
        <p>The <a
            href="https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.ARIMA.html#pmdarima.arima.ARIMA.predict">predict</a>
          method of <em>auto_arima</em> generates both <em>point predictions</em> and <em>confidence intervals</em> by
          setting the \(\alpha\) parameter as shown below:</p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/6052f08aa7f944d8b629cd8b3cb5058b.js"></script>

        <p>The next graph shows both point predictions and confidence intervals (orange area). The
          former
          represent the most likely values according to the model, while the latter give us a measure of uncertainty.
        </p>
        <figure><img src="https://alagrine.github.io/images/P6/8_ARIMA_preds.png" />
        </figure>

        <p>If we zoom in, we see that the ARIMA predictions follow the test data. They are slightly behind.</p>
        <p>Each prediction in the ARIMA(2,1,2) forecast is off by an average of \($391.5\), which is worse than the
          Naive
          predictions.</p>
        <br>
        <h2 id="forecast-with-arima-model">Forecast 14 steps ahead with ARIMA</h2>

        <p>Fitting the ARIMA model to the full data yields the following results:</p>
        <ul>
          <li>The full search (stepwise=False) has found that an ARIMA(2,1,3) yields the lowest AIC value</li>
          <li>The automated stepwise selection has identified an ARIMA(0,1,0) model, which is equivalent to a random
            walk.</li>
        </ul>
        <p>We will make forecasts using ARIMA(2,1,3).</p>
        <blockquote>
          <p>Note that for each forecast, we will update our ARIMA model with this new prediction using the
            <a href="https://alkaline-ml.com/pmdarima/refreshing.html">update</a> function.
          </p>
        </blockquote>
        <figure><img src="https://alagrine.github.io/images/P6/12-ARIMA-forecasts.png" />
        </figure>

        <p>The plot shows that the mean forecasts look very similar to what we would get with a random walk. That is,
          including the AR and MA terms (p=2 and q=3) has made little difference to the point forecasts.</p>
        <br>
        <p>So far, we have used classical approaches to forecast Bitcoin prices. We found that the exponential
          smoothing
          methods (SES and Holt damped) slightly outperform the Naive model, which yields better results than the
          ARIMA
          model. This shows how difficult it is to beat the Naive model in open systems such as stock or crypto
          markets.
        </p>
        <p>In the next section we will use a different approach: deep learning models using TensorFlow.</p>

        <div class="mb-4">
          <hr class="rounded">
        </div>

        <h1 id="bitcoin-price-forecasting-with-deep-learning-models-using-tensorflow">Bitcoin price forecasting with
          deep learning models using Tensorflow</h1>
        <p>We will be building a variety of artificial neural networks (including linear, CNN and RNN models) to make
          time series predictions using a window of \(p\) successive values of the data.</p>
        <p>A deep learning model with \(p\) lagged inputs and \(0\) nodes in the hidden layer is equivalent to an
          ARIMA(p,0,0).</p>
        <p>While moving averages, exponential smoothing and Arima can only use <strong>univariate</strong> time
          series,
          deep learning models can use <strong>multivariate</strong> time series.</p>
        <br>
        <h2 id="feature-engineering">Feature engineering</h2>
        <p>To identify potential trend changes, We will feed our models with the following key features:</p>
        <ol>
          <li>
            <p>Moving Average Convergence Divergence <a href="https://en.wikipedia.org/wiki/MACD">MACD</a>, a momentum
              indicator which is
              commoly calculated by subtracting the 26-period EMA from the 12-period EMA.</p>
            <p>We can use the <a
                href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html">exponentially
                weighted mean</a> (ewm) function from the Pandas library as shown in the snippet below:</p>
            <script type="application/javascript"
              src="https://gist.github.com/AlaGrine/d09fd7bbd601b16cfe4226d37239a178.js"></script>

          </li>
          <li>
            <p>The <a href="https://www.investopedia.com/terms/b/block-reward.asp">block reward size</a>,
              the number of bitcoins awarded for mining a block of bitcoin, which is halved after the creation of every
              210,000 blocks
              (i.e. 1% of bitcoins). <br> The halving of this factor is
              associated
              with price increases, as shown in the next chart.</p>
            <figure><img src="https://alagrine.github.io/images/P6/9-block-reward.png" />
            </figure>
          </li>

        </ol>


        <br>
        <h2 id="scale-the-data">Scale the data</h2>
        <p>As these features do not have the same range, it is important to scale them before training our neural
          network.
        </p>
        <p>A common way to scale them is the <code>MinMaxScaler</code> which transforms each feature into the range
          between 0 and 1. This transformation should be computed using only the <strong>training</strong> data set as
          follows:
        </p>
        <pre tabindex="0" style="overflow: hidden;"><code>SPLIT_SIZE = 0.8
n = <span style="color:cyan;">len</span>(bitcoin_data)

train_df = bitcoin_data[0:<span style="color:cyan;">int</span>(n*SPLIT_SIZE)]
train_min = train_df<span style="color:cyan;">.min()</span>
train_max = train_df<span style="color:cyan;">.max()</span>

bitcoin_data = (bitcoin_data - train_min) / (train_max-train_min) 
</code></pre>
        <p>That is, our neural network will predict the scaled prices. To get back to real BTC prices, we can use the
          following code:</p>
        <pre tabindex="0" style="overflow: hidden;"><code># Scaled price predictions
preds = tf.squeeze(model<span style="color:cyan;">.predict</span>(test_dataset,verbose=0))

# Get back to real BTC prices:
preds_BTC = (train_max[&#39;Price&#39;]-train_min[&#39;Price&#39;])*preds+train_min[&#39;Price&#39;]
</code></pre>
        <br>
        <h2 id="windowing">Windowing</h2>
        <p>Windowing is used to turn a time series dataset into a <em>supervised learning problem</em>.</p>
        <p>We need to set two hyperparameters:</p>
        <ul>
          <li><code>HORIZON</code>: number of time steps to predict in the future;</li>
          <li><code>WINDOW_SIZE</code>: number of time steps from the past used to predict the HORIZON.</li>
        </ul>
        <p>For example, with a horizon of 1 and a window size of 7, we can predict the scaled price based on the
          previous week&rsquo;s values.</p>

        <p>The following code block illustrates how to create windows of consecutive samples from the data:</p>

        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/016262b053a2b15d18bfb22ca9f89a30.js"></script>

        <br>
        <h2 id="modelling-and-evaluation">Modelling and Evaluation</h2>
        <p>We will be building the following model structure:</p>
        <pre tabindex="0" style="overflow: hidden;"><code>Input (windowed dataset) -&gt; <span style="color:rgb(221, 26, 149);">Custom_Layers</span> -&gt; Output (BTC price)
</code></pre>
        <p>The main component we&rsquo;ll be changing throughout is the <code>Custom_Layers</code> component.
          Specifically, we will build the following:</p>
        <ul>
          <li><strong>Model 1</strong>: A simple Dense model</li>
          <li><strong>Model 2</strong>: LSTM model</li>
          <li><strong>Model 3</strong>: GRU model</li>
          <li><strong>Model 4</strong>: 1D Convolutional Neural Network</li>
        </ul>
        <p>Each model is built using the following function:</p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/25980c4f7a49fae7e8f79cc6737df879.js"></script>

        <p>For example, to create a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense">Dense</a>
          model, the following layers are used:</p>
        <pre tabindex="0"><code>Dense_layers = [layers<span style="color:cyan;">.Flatten()</span>,
                layers<span style="color:cyan;">.Dense(32)</span>]
</code></pre>
        <figure><img src="https://alagrine.github.io/images/P6/10-MAE_tensorflow.png" />
        </figure>

        <p>Clearly, the smaller the window size, the better the MAE. The optimal window size for all models is
          <strong>1</strong>.
        </p>
        <p>For window sizes greater than 1, the <em>GRU</em> model yields the best results.</p>

        <br>
        <h2 id="compare_results">Compare results</h1>
          <p>Here is a comparison of the performance of all the models that we have built so far.</p>
          <figure><img src="https://alagrine.github.io/images/P6/20_MAE_models.png" />
          </figure>

          <p>The <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D">Conv1D</a> and <a
              href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense">Dense</a> models (with one layer
            and 32 neurons) yield the best results
            with an MAE of \(370\), ahead of the SES and GRU models, with an MAE of \(377\).
          </p>

          <div class="mb-4">
            <hr class="rounded">
          </div>

          <h1 id="conclusion">Conclusion</h1>
          <p>In this project, I have been forecasting a series of daily closing prices for Bitcoin.</p>
          <p>I built a variety of models, from classics like <em>ARIMA</em> and <em>exponential smoothing</em> to deep
            neural networks. The
            latter were used with multivariate time series, as in addition to the historical price of Bitcoin, we fed
            them
            with other features such as MACD, which slightly improved the performance of our model.</p>
          <p>I found that these models slightly outperformed the Naive model. This shows how difficult it is to make
            predictions in open systems like stock or crypto markets.</p>

          <p>Throughout this project, I have used pmdarima and Tensorflow.

            There are other time series libraries we can use, such as <a
              href="https://www.sktime.net/en/stable">sktime</a>,
            <a href="https://facebook.github.io/prophet/docs/quick_start.html#python-api">prophet</a> and <a
              href="https://linkedin.github.io/greykite/">greykite</a>
            to name a few.
          </p>
          <br>
          <p><a href="https://github.com/AlaGrine/Forecasting_Bitcoin_Price_Series">Link to GitHub
              repository</a></p>

          <span class="badge badge-pill badge-success">Time series analysis</span>
          <span class="badge badge-pill badge-success">Exponontiel Smoothing</span>
          <span class="badge badge-pill badge-success">ARIMA</span>
          <span class="badge badge-pill badge-success">Tesnsorflow</span>
          <ul class="pa0">

          </ul>
          <div class="mt6 instapaper_ignoref">


          </div>
      </div>

      <aside class="w-30-l mt6-l">




      </aside>

    </article>

  </main>
  <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
    <div class="flex justify-between">
      <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://alagrine.github.io/">
        &copy; Ala Eddine GRINE 2023
      </a>
      <div>
        <div class="ananke-socials">


          <a href="https://github.com/AlaGrine" target="_blank" rel="noopener"
            class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
            title="GitHub link" aria-label="follow on GitHub——Opens in a new window">

            <span class="icon"><svg style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512"
                xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z" />
              </svg>
            </span>

            <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span></a>


          <a href="https://www.linkedin.com/in/ala-eddine-grine" target="_blank" rel="noopener"
            class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
            title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window">

            <span class="icon"><svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"
                xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span>

            <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span></a>


          <a href="https://medium.com/@alaeddine.grine" target="_blank" rel="noopener"
            class="medium ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
            title="Medium link" aria-label="follow on Medium——Opens in a new window">

            <span class="icon"><svg style="enable-background:new 0 0 170 170;" version="1.1" viewBox="0 0 170 170"
                xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M46.5340803,65.2157554 C46.6968378,63.6076572 46.0836,62.018231 44.8828198,60.93592 L32.6512605,46.2010582 L32.6512605,44 L70.6302521,44 L99.9859944,108.380952 L125.794585,44 L162,44 L162,46.2010582 L151.542017,56.2281011 C150.640424,56.9153477 150.193188,58.0448862 150.380019,59.1628454 L150.380019,132.837155 C150.193188,133.955114 150.640424,135.084652 151.542017,135.771899 L161.755369,145.798942 L161.755369,148 L110.38282,148 L110.38282,145.798942 L120.963119,135.527337 C122.002801,134.487948 122.002801,134.182246 122.002801,132.592593 L122.002801,73.0417402 L92.585901,147.755438 L88.6106443,147.755438 L54.3622782,73.0417402 L54.3622782,123.115814 C54.0767278,125.221069 54.7759199,127.3406 56.2581699,128.863022 L70.0186741,145.55438 L70.0186741,147.755438 L31,147.755438 L31,145.55438 L44.7605042,128.863022 C46.2319621,127.338076 46.8903838,125.204485 46.5340803,123.115814 L46.5340803,65.2157554 Z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span>

            <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span></a>

        </div>
      </div>
    </div>
  </footer>

  <!-- Add ScrollToTop button; only visible when we scroll up -->
  <div id="scrollToTopBtn" class="scrollToTopBtn"> <i class="fas fa-arrow-up"></i> Back to
    top</div>

  <script>

    window.onscroll = function () {
      if (pageYOffset >= 100) {
        document.getElementById('scrollToTopBtn').style.visibility = "visible";
      } else {
        document.getElementById('scrollToTopBtn').style.visibility = "hidden";
      }
    };
    var scrollToTopBtn = document.querySelector(".scrollToTopBtn");
    var rootElement = document.documentElement;

    let lastScrollTop = window.pageYOffset || document.documentElement.scrollTop;

    function handleScroll() {
      var scrollTotal = rootElement.scrollHeight - rootElement.clientHeight;
      const scrollTopPosition = window.pageYOffset || document.documentElement.scrollTop;

      if (scrollTopPosition > lastScrollTop) {
        // Hide button
        scrollToTopBtn.classList.remove("showBtn");
      } else if (scrollTopPosition < lastScrollTop) {
        // Show button
        scrollToTopBtn.classList.add("showBtn");
      }
      lastScrollTop = scrollTopPosition <= 0 ? 0 : scrollTopPosition;
    }

    function scrollToTop() {
      rootElement.scrollTo({
        top: 0,
        behavior: "smooth"
      });
      handleScroll();
    }
    scrollToTopBtn.addEventListener("click", scrollToTop);
    document.addEventListener("scroll", handleScroll);
  </script>


</body>

</html>