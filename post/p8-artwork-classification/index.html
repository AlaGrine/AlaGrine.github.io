<!DOCTYPE html>
<html lang="en">

<head>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
    integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" />

  <!-- Math formulas -->
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>


  <style type="text/css">
    * {
      scroll-behavior: smooth;
    }

    .scrollToTopBtn {
      position: fixed;
      top: 10%;
      left: 50%;
      transform: translate(-50%, -50%);
      padding: 5px 10px;
      background-color: #27ae60;
      color: #fff;
      border-radius: 25px;
      border-radius: 30px solid #fff;
      cursor: pointer;
      transition: all 0.5s ease 0s;
      /* keep it at the top of everything else */
      z-index: 100;
      /* hide with opacity */
      opacity: 0;

    }

    .showBtn {
      opacity: 1;
    }

    /* Rounded border */
    hr.rounded {
      margin-top: 4%;
      margin-bottom: 3%;
      border-top: 5px solid #bbb;
      border-radius: 3px;
    }

    figure {
      /* display: block; */
      margin-left: auto;
      margin-right: auto;
      text-align: center;
    }

    figcaption {
      background-color: black;
      color: white;
      font-style: italic;
      padding: 2px;
      text-align: center;
    }

    .th1,
    .td1 {
      border: 1px solid black;
      border-radius: 10px;
      padding: 5px 25px;
    }
  </style>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <title>Artwork Classification in PyTorch | Ala Eddine GRINE</title>
  <meta name="viewport" content="width=device-width,minimum-scale=1">
  <meta name="description" content="Image classification in PyTorch using Convolutional and Transformer models">
  <meta name="generator" content="Hugo 0.118.2">




  <meta name="robots" content="noindex, nofollow">



  <link rel="stylesheet" href="https://alagrine.github.io/ananke/css/main.min.css">







  <link rel="shortcut icon" href="https://alagrine.github.io/images/portfolio.png" type="image/x-icon" />






  <meta property="og:title" content="Artwork Classification in PyTorch" />
  <meta property="og:description"
    content="Image classification in PyTorch using Convolutional and Transformer models" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="https://alagrine.github.io/post/p8-artwork-classification/" />
  <meta property="article:section" content="post" />
  <meta property="article:published_time" content="2023-12-19T01:14:48-04:00" />
  <meta property="article:modified_time" content="2023-12-19T01:14:48-04:00" />
  <meta itemprop="name" content="Artwork Classification in PyTorch">
  <meta itemprop="description" content="Image classification in PyTorch using Convolutional and Transformer models">
  <meta itemprop="datePublished" content="2023-12-19T01:14:48-04:00" />
  <meta itemprop="dateModified" content="2023-12-19T01:14:48-04:00" />
  <meta itemprop="wordCount" content="1369">
  <meta itemprop="keywords" content="" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Artwork Classification in PyTorch" />
  <meta name="twitter:description"
    content="Image classification in PyTorch using Convolutional and Transformer models" />


</head>

<body class="ma0 avenir bg-near-white">






  <header class="cover bg-top" style="background-image: url('https://alagrine.github.io/images/P8/fig_Artbench.png');">
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
        <div class="flex-l justify-between items-center center">
          <a href="https://alagrine.github.io/" class="f3 fw2 hover-white no-underline white-90 dib">

            Ala Eddine GRINE

          </a>
          <div class="flex-l items-center">



            <ul class="pl0 mr3">

              <li class="list f5 f4-ns fw4 dib pr3">
                <a class="hover-white no-underline white-90" href="https://alagrine.github.io/about/"
                  title="About page">
                  About
                </a>
              </li>

              <li class="list f5 f4-ns fw4 dib pr3">
                <a class="hover-white no-underline white-90" href="https://alagrine.github.io/post/"
                  title="Projects page">
                  Projects
                </a>
              </li>

            </ul>


            <div class="ananke-socials">


              <a href="https://github.com/AlaGrine" target="_blank" rel="noopener"
                class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
                title="GitHub link" aria-label="follow on GitHub——Opens in a new window">

                <span class="icon"><svg style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512"
                    xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z" />
                  </svg>
                </span>

                <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                    viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                    xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                      style="fill-rule:evenodd;clip-rule:evenodd;" />
                  </svg>
                </span></a>


              <a href="https://www.linkedin.com/in/ala-eddine-grine" target="_blank" rel="noopener"
                class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
                title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window">

                <span class="icon"><svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"
                    xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z"
                      style="fill-rule:evenodd;clip-rule:evenodd;" />
                  </svg>
                </span>

                <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                    viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                    xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                      style="fill-rule:evenodd;clip-rule:evenodd;" />
                  </svg>
                </span></a>


              <a href="https://medium.com/@alaeddine.grine" target="_blank" rel="noopener"
                class="medium ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
                title="Medium link" aria-label="follow on Medium——Opens in a new window">

                <span class="icon"><svg style="enable-background:new 0 0 170 170;" version="1.1" viewBox="0 0 170 170"
                    xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M46.5340803,65.2157554 C46.6968378,63.6076572 46.0836,62.018231 44.8828198,60.93592 L32.6512605,46.2010582 L32.6512605,44 L70.6302521,44 L99.9859944,108.380952 L125.794585,44 L162,44 L162,46.2010582 L151.542017,56.2281011 C150.640424,56.9153477 150.193188,58.0448862 150.380019,59.1628454 L150.380019,132.837155 C150.193188,133.955114 150.640424,135.084652 151.542017,135.771899 L161.755369,145.798942 L161.755369,148 L110.38282,148 L110.38282,145.798942 L120.963119,135.527337 C122.002801,134.487948 122.002801,134.182246 122.002801,132.592593 L122.002801,73.0417402 L92.585901,147.755438 L88.6106443,147.755438 L54.3622782,73.0417402 L54.3622782,123.115814 C54.0767278,125.221069 54.7759199,127.3406 56.2581699,128.863022 L70.0186741,145.55438 L70.0186741,147.755438 L31,147.755438 L31,145.55438 L44.7605042,128.863022 C46.2319621,127.338076 46.8903838,125.204485 46.5340803,123.115814 L46.5340803,65.2157554 Z"
                      style="fill-rule:evenodd;clip-rule:evenodd;" />
                  </svg>
                </span>

                <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                    viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                    xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                      style="fill-rule:evenodd;clip-rule:evenodd;" />
                  </svg>
                </span></a>

            </div>

          </div>
        </div>
      </nav>

      <div class="tc-l pv6 ph3 ph4-ns">

        <div class="f2 f1-l fw2 white-90 mb0 lh-title">Artwork Classification in PyTorch</div>

        <div class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
          Image classification in PyTorch using Convolutional and Transformer models
        </div>


      </div>
    </div>
  </header>



  <main class="pb7" role="main">


    <article class="flex-l flex-wrap justify-between mw8 center ph3">
      <header class="mt4 w-100">
        <aside class="instapaper_ignoref b helvetica tracked">

          PROJECTS
        </aside>











        <div id="sharing" class="mt3 ananke-socials">


          <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://alagrine.github.io/post/p8-artwork-classification/&amp;title=Artwork%20Classification%20in%20PyTorch"
            class="ananke-social-link linkedin no-underline" aria-label="share on LinkedIn">

            <span class="icon"> <svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"
                xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span>

          </a>

        </div>


        <h1 class="f1 athelas mt3 mb1">Artwork Classification in PyTorch</h1>



        <time class="f6 mv4 dib tracked" datetime="2023-12-19T01:14:48-04:00">December 19, 2023</time>




      </header>
      <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-100">
        <h1 id="project-overview">Project Overview:</h1>
        <p>In this project, we will classify artworks using the <a
            href="https://github.com/liaopeiyuan/artbench">artbench</a> dataset. The dataset includes 60,000 images of
          artworks from 10 different artistic styles, including paintings, murals, and sculptures from the 14th to the
          21st century. Each style has 5,000 training images and 1,000 test images.</p>
        <p>We will use PyTorch to predict classes, employing <strong>Convolutional</strong> and
          <strong>Transformer</strong> models. Specifically, we will
          leverage and fine-tune the <a
            href="https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.efficientnet_b2">EfficientNet-B2</a>
          and <a
            href="https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html?highlight=vit#torchvision.models.vit_b_16">ViT-B16</a>
          pre-trained models.
        </p>
        <p>To speed up our model training, we will use two free Kaggle GPU T4 accelerators. Free GPUs are also available
          on Google Colab.</p>
        <p>Finally, we will create a <a href="https://www.gradio.app/">Gradio</a> demo and deploy the app to <a
            href="https://huggingface.co/spaces">HuggingFace Spaces</a>.</p>

        <div class="mb-4">
          <hr class="rounded">
        </div>

        <h1 id="getting-data">Getting data</h1>
        <p>There are three versions of the <em>artbench</em> dataset, each with a different resolution: 32x32, 256x256,
          and the original.</p>
        <p>We will download the 256x256 ImageFolder with train-test split from <a
            href="https://github.com/liaopeiyuan/artbench">here</a>.</p>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code><span style="color:greenyellow;"># download the artbench dataset</span>
import wget
wget.<span style="color:cyan;">download</span>(url=&#34;https://artbench.eecs.berkeley.edu/files/artbench-10-imagefolder-split.tar&#34;,
              out = FLAGS[&#39;datadir&#39;])

<span style="color:greenyellow;"># Uncompress the tar file</span>
import tarfile
file = tarfile.<span style="color:cyan;">open</span>(data_path / &#34;artbench-10-imagefolder-split.tar&#34;)
file.<span style="color:cyan;">extractall</span>(data_path)
file.<span style="color:cyan;">close</span>()  
</code></pre>

        <div class="mb-4">
          <hr class="rounded">
        </div>
        <h1 id="data-preparation">Data preparation</h1>
        <p>This section covers the transformation of images using <em>torchvision.transforms</em>, loading image data,
          and
          transforming loaded images into DataLoaders.</p>
        <h2 id="transforming-data-with-torchvisiontransforms">Transforming data with torchvision.transforms</h2>
        <p>The <em>torchvision.transforms</em> module is used here to <a
            href="https://pytorch.org/vision/stable/transforms.html">transform and augment images</a>. A series of
          transforms is
          created using the <em>transforms.Compose</em> method.</p>
        <p>The images are resized using <em>transforms.Resize()</em> and converted to tensor and scaled between 0
          and 1
          using <em>transforms.ToTensor()</em>.</p>
        <p>To prevent overfitting, we will use <a
            href="https://pytorch.org/vision/main/generated/torchvision.transforms.TrivialAugmentWide.html">transforms.TrivialAugmentWide()</a>.
          This is an automatic augmentation method
          that transforms an image using a random transform from a given list with a random strength number between 0
          and 31. We have found that a strength of 8 provides better accuracy and loss than higher magnitudes.</p>


        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code>data_transform = transforms.<span style="color:cyan;">Compose()</span>([
transforms.<span style="color:cyan;">Resize</span>(size=(FLAGS[&#39;IMAGE_SIZE&#39;], FLAGS[&#39;IMAGE_SIZE&#39;])),
transforms.<span style="color:cyan;">TrivialAugmentWide</span>(num_magnitude_bins=8), <span style="color:greenyellow;"># Data Augmentation</span>
transforms.<span style="color:cyan;">ToTensor</span>()
])
</code></pre>

        <p>In this example, we pass an image through the transform pipeline to obtain the following transformed output:
        </p>

        <figure><img src="https://alagrine.github.io/images/P8/fig_data_augmentation.png" />
        </figure>

        <p>This will increase the diversity of the training data and prevent overfitting.</p>
        <br>
        <h2 id="loading-images-using-imagefolder">Loading images using ImageFolder</h2>
        <p>The <a
            href="https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html">torchvision.datasets.ImageFolder</a>
          class is used to convert image data into a Dataset.
          The <em>transform</em> argument should be used to pass the transform pipeline, while the <em>root</em>
          argument should contain
          the path of the target image directory.</p>

        <p>The images will be in the form of a tensor with shape
          [3,224,224], and the labels will be integers.</p>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code>from torchvision import datasets

train_data = datasets.<span style="color:cyan;">ImageFolder</span>(root=image_path / &#34;train&#34;, transform=data_transform)
test_data = datasets.<span style="color:cyan;">ImageFolder</span>(root=image_path / &#34;test&#34;, transform=data_transform)
</code></pre>
        <br>
        <h2 id="turning-loaded-images-to-dataloaders">Turning loaded images to DataLoaders</h2>
        <p>The last step in preparing the data is to convert our Dataset into a DataLoader using <a
            href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">torch.utils.data.DataLoader()</a>.
          This returns a Python
          iterable that we will use to iterate over batches. This method also supports automatic shuffling, and memory
          pinning.</p>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code>train_DataLoader = <span style="color:cyan;">DataLoader</span>(dataset=train_data,
                              batch_size=FLAGS[&#39;batch_size&#39;],
                              num_workers=FLAGS[&#39;num_workers&#39;],
                              shuffle=True)
test_DataLoader = <span style="color:cyan;">DataLoader</span>(dataset=train_data,
                             batch_size=FLAGS[&#39;batch_size&#39;],
                             num_workers=FLAGS[&#39;num_workers&#39;],
                             shuffle=False)
</code></pre>
        <p>So far, the artbench images have been transformed into tensors suitable for use by computer vision models.
        </p>
        <p>We will now create several models to predict artwork classes, starting with a simple <em>TinyVGG</em>
          architecture.
        </p>

        <div class="mb-4">
          <hr class="rounded">
        </div>
        <h1 id="tinyvgg">TinyVGG</h1>
        <p>Here, we replicate the TinyVGG model from the <a href="https://poloclub.github.io/cnn-explainer/">CNN
            Explainer</a> website to create a <em>Convolutional Neural Network</em> (CNN).</p>
        <p>The TinyVGG model comprises 4 convolutional layers, each with 10 neurons and three learned kernels. A
          <em>ReLU</em>
          activation is applied
          after each Conv2d layer to introduce non-linearity.
        </p>
        <p>To reduce the number of parameters and prevent overfitting, <em>Max-Pooling</em> is used.</p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/2fe2e11d9bc22afd96ea0beae3bc3104.js"></script>

        <p>To speed up training, we will use a GPU. Therefore, we need to define the device we will put our model on:
        </p>

        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code>device = "<span style="color:coral;">cuda</span>" if torch.cuda.<span style="color:cyan;">is_available()</span> else "<span style="color:coral;">cpu</span>"
model_0 = TinyVGG(input_shape=3, <span style="color:greenyellow;"># number of color channels</span> 
                  hidden_units=10,
                  output_shape=len(train_data.classes))
model_0.<span style="color:cyan;">to(device)</span>
</code></pre>
        <br>
        <h2 id="create-train-and-test-loop-functions">Create train and test loop functions</h2>
        <p>Let&rsquo;s create three functions to train and evaluate our models. These functions will include a train
          step,
          a test step, and a function to combine them and create a train and test loop.</p>

        <h3 id="train_step_function">Train step function</h3>

        <p>First, we activate the training mode of the model with the command <em> model.train()</em>. Next, we
          iterate through
          the batches in the train_DataLoader, transferring the data to the target device, calculating and
          accumulating the loss and accuracy, and performing the gradient descent. Finally, once we have accumulated all
          the batches, we adjust the metrics to
          obtain the average loss and accuracy per
          batch. </p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/49a9dc832cd4f0f4edf9d94c2f40711f.js"></script>

        <h3 id="test_step_function">Test step function</h3>
        <p>Here, we activate the model's eval mode by using the command <em>model.eval()</em> and use the
          <em>torch.inference_mode()</em> context manager.
        </p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/142bcbf2b701b9f0e50722b7a9ccc0f9.js"></script>
        <p>As you can see, the core of the test step looks very similar to the training step. The main difference is
          that we're not running gradient descent.</p>

        <h3 id="train_test_loop">Train and test loop</h3>
        <p> To bring everything together, we create a function for the train and test loop and add a progress bar for
          the number of epochs using the <em>tqdm</em> library.</p>
        <p>We will use <em>CrossEntropyLoss</em> as the loss function and <em>Adam</em> as the optimizer. The learning
          rate for the Adam
          optimizer is set to 0.001 by default.</p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/3e4f08f1b3704a2bd2e974dd1c90ed0c.js"></script>

        <br>
        <h2 id="evaluate-the-tinyvgg-model">Evaluate the TinyVGG model</h2>
        <p>Training the TinyVGG for 5 epohs yields the following results.</p>
        <figure><img src="https://alagrine.github.io/images/P8/fig_Tiny_VGG.jpg" />
        </figure>

        <p>The accuracy of the TinyVGG model was 30.8%, which is still better than guessing, which would be an accuracy
          of 10%.</p>
        <p>It may be worth considering training the model for a longer period to see if there is any improvement.</p>
        <div class="mb-4">
          <hr class="rounded">
        </div>
        <h1 id="convolutional-model---efficientnet_b2-feature-extraction">Convolutional model: EfficientNet_B2 - Feature
          Extraction</h1>
        <p>We will leverage the <a
            href="https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.efficientnet_b2">EfficientNet-B2</a>
          pre-trained on <em>ImageNet</em> and use its underlying learned representations to classify artwork images.
        </p>
        <p>For feature extraction, we freeze all layers of the model and adjust the output layers to have 10 classes
          instead of the 1000 classes of the <em>ImageNet</em> dataset. We then train the model for 5 epochs on 20% of
          the data.
        </p>
        <p>The snippet below displays the code for creating an <em>EfficientNet_B2</em> feature extractor.</p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/7236014a7d4c5483b8f7f7133cd3ebce.js"></script>

        <p>Before training the model, we need to create new Dataloaders using the <em>EfficientNet_B2</em> transforms.
        </p>
        <figure><img src="https://alagrine.github.io/images/P8/fig_Efficient_Net_B2_20percent.jpg" />
        </figure>

        <p>The <em>EfficientNet_B2</em> feature extractor achieved an accuracy of over 46%, surpassing that of
          <em>TinyVGG</em>, despite
          using only 20% of the data for training. This demonstrates the effectiveness of Transfer Learning.
        </p>
        <p>The loss curves suggest that the metrics (loss and accuracy) would further improve with additional epochs of
          training. However, we will <em>fine-tune</em> the feature extraction model.</p>

        <div class="mb-4">
          <hr class="rounded">
        </div>
        <h1 id="efficientnet_b2-fine-tuning">EfficientNet_B2: Fine-tuning</h1>
        <p>To fine-tune the EfficientNet_B2, we will unfreeze all layers and train the model for a few more epochs. The
          following steps will be taken:</p>
        <ol>
          <li>Create a new instance of <eme>EfficientNet_B2</eme> with all its layers trainable.</li>
          <li>Load the <em>state_dict</em> of our saved feature extractor model. This will update the new instance of
            our model
            with trained weights.</li>
          <li>Train the model for an additional 5 epochs on the full dataset, as fine-tuning a model usually works
            better with more data. To ensure that the model retains its previously learned features without significant
            alterations, we will use a smaller learning rate. The initial learning rate will be divided by 10.</li>
        </ol>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code><span style="color:greenyellow;"># 1. Create a new instance of EfficientNet_B2 with all its layers trainable</span>
model_EfficientNet_B2,effnetb2_transforms = create_effnetb2_model(num_classes=10,
                                                                  is_TrivialAugmentWide = True,
                                                                  freeze_layers=False)

<span style="color:greenyellow;"># 2. Load the state_dict of our saved feature extractor model</span>
MODEL_SAVE_PATH = PATH_models / &#34;EfficientNet_B2_20percent.pth&#34;
model_EfficientNet_B2.<span style="color:cyan;">load_state_dict</span>load_state_dict(torch.<span style="color:cyan;">load</span>(f=MODEL_SAVE_PATH))

<span style="color:greenyellow;"># 3. Fine-tune the model</span>
results_EfficientNet_B2_FT = train_and_evaluate(model=model_EfficientNet_B2,
                                                train_dataloader=train_DataLoader,
                                                test_dataloader=test_DataLoader,
                                                epochs=5,
                                                learning_rate=<span style="color:cyan;">0.0001</span>)<span style="color:coral;"> # Divide the learning rate by 10.</span>
</code></pre>
        <figure><img src="https://alagrine.github.io/images/P8/fig_EfficientNet_B2_FT.jpg" />
        </figure>

        <p>Our model&rsquo;s accuracy on the test dataset has increased to <strong>67%</strong> after fine-tuning with
          all available data. Training for a longer period could further improve accuracy.</p>
        <p>The accuracy of <em>ukiyo-e</em> is the highest at 99%, while <em>realism</em> has the lowest accuracy at
          approximately 50%.
          The correlation matrix is shown below:</p>
        <figure><img src="https://alagrine.github.io/images/P8/fig_effNet_confusionMatrix.jpg" />
        </figure>

        <div class="mb-4">
          <hr class="rounded">
        </div>
        <h1 id="transformer-model---vit_b16">Transformer model - ViT_B16</h1>
        <p><em>Transformers</em> is a state-of-the-art architecture originally designed to work on one-dimensional (1D)
          text sequences. The Vision Transformer <a href="https://arxiv.org/abs/2010.11929">ViT</a> architecture is
          designed to adapt the original Transformer architecture to vision use cases. While a convolutional neural
          network (CNN) uses convolutions as learning layers, a Transformer architecture
          uses <a href="https://arxiv.org/abs/1706.03762"> attention layers</a>.</p>
        <p>I replicated the <a href="https://arxiv.org/abs/2010.11929">ViT paper</a> to enhance comprehension
          of the Transformer architecture. <a
            href="https://github.com/AlaGrine/Artwork_classification_in_PyTorch/blob/main/Create_ViT_from_scratch.ipynb">Here</a>
          is the code to build the ViT model from scratch using PyTorch.</p>
        <p>
          To avoid training the ViT model from scratch, let's utilize transfer learning and leverage the pre-trained <a
            href="https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html?highlight=vit#torchvision.models.vit_b_16">ViT-B16</a>
          available on torchvision.models.
        </p>
        <p>The code snippet below shows how to create a <em>ViT_B16</em> model in PyTorch. The
          <em>ViT transforms</em> are also returned, to which we add <em>TrivialAugmentWide()</em> for data
          augmentation.
        </p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/421a6fad2da3151ff6f24a39b9876dcf.js"></script>

        <p>Like <em>EfficientNet</em>, we start by applying a feature extractor to 20% of the data. Then, we fine-tune
          the model
          for 5 epochs on the entire dataset. Additionally, we must generate new DataLoaders using the <em>Vit_B16</em>
          transforms.
        </p>
        <figure><img src="https://alagrine.github.io/images/P8/fig_ViT_B16_FT.jpg" />
        </figure>

        <div class="mb-4">
          <hr class="rounded">
        </div>
        <h1 id="compare-model-results">Compare model results</h1>
        <figure><img src="https://alagrine.github.io/images/P8/fig_inference-speed-vs-accuracy.jpg" />
        </figure>

        <p>Our <em>EfficientNet_B2</em> outperforms the <em>ViT_B16</em> model in all performance metrics. It achieves
          the highest accuracy,
          lowest loss, smallest size, and shortest prediction time per image.</p>
        <p>In the next section we will create and diploy a <em>Gradio</em> demo. The <em>EfficientNet_B2</em> will be
          our ML model.</p>
        <div class="mb-4">
          <hr class="rounded">
        </div>
        <h1 id="create-and-deploy-a-gradio-application">Create and deploy a Gradio application</h1>
        <p><a href="https://www.gradio.app/">Gradio</a> provides a very useful <strong>Interface</strong> class to
          easily create a web-based demo around a machine learning model, desired input and output components.</p>
        <p>For our ArtWork classifier, our ML model is EfficienNet_B2, our inputs are images of artworks, and our
          outputs are their classes (Baroque, Realism, Impressionism, &hellip;).</p>
        <p>We will upload our Gradio app to <a href="https://huggingface.co/spaces">Hugging Face Spaces</a>, a resource
          widely used by the machine learning community.</p>
        <h2 id="create-a-deployable-gradio-demo">Create a deployable Gradio demo</h2>
        <p>The structure of the deployable application is the following:</p>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code> gradio_demo/
    ├── EfficientNet_B2_FT.pth
    ├── app.py
    ├── examples/
    │   ├── example_1.jpg
    │   ├── example_2.jpg
    │   └── example_3.jpg
    ├── model.py
    └── requirements.txt
</code></pre>
        <p>Where:</p>
        <ul>
          <li><code>EfficientNet_B2_FT.pth</code> is our trained PyTorch model file.</li>
          <li><code>app.py</code> contains our Gradio app.</li>
          <li><code>examples/</code> contains example images to use with our Gradio app.</li>
          <li><code>model.py</code> contains the <em>create_effnetb2_model</em> function that we created
            earlier.</li>
          <li><code>requirements.txt</code> contains the dependencies to run our app such as <em>torch</em>,
            <em>torchvision</em> and <em>gradio</em>.
          </li>
        </ul>
        <p>To create the main file (app.py) of the Gradio demo, we can use the following code:</p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/8e973f87f55a2d17a9bd1802c6f2f066.js"></script>
        <br>
        <h2 id="deploy-the-gradio-app-to-huggingface-spaces">Deploy the Gradio app to HuggingFace Spaces</h2>
        <p>To deploy the Gradio app to HuggingFace Spaces, we will follow the following steps:</p>
        <ol>
          <li>
            <p>Create a new space (ie. code repository). Space name = [SPACE_NAME].</p>
          </li>
          <li>
            <p>Select Gradio as the Space SDK and CPU basic (free) as Space hardware.</p>
          </li>
        </ol>
        Then, we follow the standard git workflow:
        <ol start="3">
          <li>
            <p>Clone the repo locally: <code>git clone https://huggingface.co/spaces/[USERNAME]/[SPACE_NAME]</code></p>
          </li>
          <li>
            <p>Copy the contents of <strong>grafio_demo</strong> folder to the <strong>clonded repo</strong> folder.</p>
          </li>
          <li>
            <p>Passwords are no longer accepted as a way to authenticate command-line Git operations. We will use a
              personal access token as explained
              <a href="https://huggingface.co/blog/password-git-deprecation">here</a>.
            </p>
            <p>
              <code>git remote set-url origin https://[USERNAME]:[TOKEN]@huggingface.co/spaces/[USERNAME]/[SPACE_NAME]</code>
            </p>
          </li>
          <li>
            <p><code>git add .</code></p>
          </li>
          <li>
            <p><code>git commit -m &quot;first commit&quot;</code></p>
          </li>
          <li>
            <p><code>git push</code></p>
          </li>
        </ol>
        <p>
          To embed the Gradio demo in a notebook as an
          <a href="https://gradio.app/sharing_your_app/#embedding-with-iframes">iframe</a>, we can use <a
            href="https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.IFrame">IPython.display.IFrame</a>
          and a link to our space, as demonstrated below:
        </p>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code>from IPython.display import IFrame
<span style="color:cyan;">IFrame</span>(src=&#34;<span style="color:greenyellow;">https://hf.space/embed/AlaGrine/Artwork_classifier</span>&#34;, width=900, height=750)
</code></pre>
        <p>You can find the app <a href="https://huggingface.co/spaces/AlaGrine/Artwork_classifier">here</a>.</p>
        <figure><img src="https://alagrine.github.io/images/P8/fig_gradio_demo.png" />
        </figure>

        <div class="mb-4">
          <hr class="rounded">
        </div>
        <h1 id="conclusion">Conclusion</h1>
        <p>In this project, we classified artworks using the artbench dataset and built various models, including
          convolutional and transformer-based ones. We fine-tuned the EfficientNet_B2 and ViT_B16 models, which
          significantly improved performance, achieving 67% accuracy.</p>
        <p>Finally, we created a Gradio demo and deployed the app to HuggingFace Spaces.</p>
        <br>
        <p><a href="https://huggingface.co/spaces/AlaGrine/Artwork_classifier">Link to Gradio app on HuggingFace
            Spaces</a>.</p>
        <p><a href="https://github.com/AlaGrine/Artwork_classification_in_PyTorch">Link to GitHub repository</a></p>


        <span class="badge badge-pill badge-success">PyTorch</span>
        <span class="badge badge-pill badge-success">Vision Transformer</span>
        <span class="badge badge-pill badge-success">EfficientNet</span>
        <span class="badge badge-pill badge-success">Gradio demo</span>


        <ul class="pa0">

        </ul>
        <div class="mt6 instapaper_ignoref">


        </div>
      </div>

      <aside class="w-30-l mt6-l">




      </aside>

    </article>

  </main>
  <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
    <div class="flex justify-between">
      <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://alagrine.github.io/">
        &copy; Ala Eddine GRINE 2023
      </a>
      <div>
        <div class="ananke-socials">


          <a href="https://github.com/AlaGrine" target="_blank" rel="noopener"
            class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
            title="GitHub link" aria-label="follow on GitHub——Opens in a new window">

            <span class="icon"><svg style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512"
                xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z" />
              </svg>
            </span>

            <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span></a>


          <a href="https://www.linkedin.com/in/ala-eddine-grine" target="_blank" rel="noopener"
            class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
            title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window">

            <span class="icon"><svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"
                xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span>

            <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span></a>


          <a href="https://medium.com/@alaeddine.grine" target="_blank" rel="noopener"
            class="medium ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
            title="Medium link" aria-label="follow on Medium——Opens in a new window">

            <span class="icon"><svg style="enable-background:new 0 0 170 170;" version="1.1" viewBox="0 0 170 170"
                xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M46.5340803,65.2157554 C46.6968378,63.6076572 46.0836,62.018231 44.8828198,60.93592 L32.6512605,46.2010582 L32.6512605,44 L70.6302521,44 L99.9859944,108.380952 L125.794585,44 L162,44 L162,46.2010582 L151.542017,56.2281011 C150.640424,56.9153477 150.193188,58.0448862 150.380019,59.1628454 L150.380019,132.837155 C150.193188,133.955114 150.640424,135.084652 151.542017,135.771899 L161.755369,145.798942 L161.755369,148 L110.38282,148 L110.38282,145.798942 L120.963119,135.527337 C122.002801,134.487948 122.002801,134.182246 122.002801,132.592593 L122.002801,73.0417402 L92.585901,147.755438 L88.6106443,147.755438 L54.3622782,73.0417402 L54.3622782,123.115814 C54.0767278,125.221069 54.7759199,127.3406 56.2581699,128.863022 L70.0186741,145.55438 L70.0186741,147.755438 L31,147.755438 L31,145.55438 L44.7605042,128.863022 C46.2319621,127.338076 46.8903838,125.204485 46.5340803,123.115814 L46.5340803,65.2157554 Z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span>

            <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span></a>

        </div>
      </div>
    </div>
  </footer>

  <!-- Add ScrollToTop button; only visible when we scroll up -->
  <div id="scrollToTopBtn" class="scrollToTopBtn"> <i class="fas fa-arrow-up"></i> Back to
    top</div>

  <script>

    window.onscroll = function () {
      if (pageYOffset >= 100) {
        document.getElementById('scrollToTopBtn').style.visibility = "visible";
      } else {
        document.getElementById('scrollToTopBtn').style.visibility = "hidden";
      }
    };
    var scrollToTopBtn = document.querySelector(".scrollToTopBtn");
    var rootElement = document.documentElement;

    let lastScrollTop = window.pageYOffset || document.documentElement.scrollTop;

    function handleScroll() {
      var scrollTotal = rootElement.scrollHeight - rootElement.clientHeight;
      const scrollTopPosition = window.pageYOffset || document.documentElement.scrollTop;

      if (scrollTopPosition > lastScrollTop) {
        // Hide button
        scrollToTopBtn.classList.remove("showBtn");
      } else if (scrollTopPosition < lastScrollTop) {
        // Show button
        scrollToTopBtn.classList.add("showBtn");
      }
      lastScrollTop = scrollTopPosition <= 0 ? 0 : scrollTopPosition;
    }

    function scrollToTop() {
      rootElement.scrollTo({
        top: 0,
        behavior: "smooth"
      });
      handleScroll();
    }
    scrollToTopBtn.addEventListener("click", scrollToTop);
    document.addEventListener("scroll", handleScroll);
  </script>


</body>

</html>