<!DOCTYPE html>
<html lang="en">

<head>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
    integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" />

  <!-- Math formulas -->
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>


  <style type="text/css">
    * {
      scroll-behavior: smooth;
    }

    .scrollToTopBtn {
      position: fixed;
      top: 10%;
      left: 50%;
      transform: translate(-50%, -50%);
      padding: 5px 10px;
      background-color: #27ae60;
      color: #fff;
      border-radius: 25px;
      border-radius: 30px solid #fff;
      cursor: pointer;
      transition: all 0.5s ease 0s;
      /* keep it at the top of everything else */
      z-index: 100;
      /* hide with opacity */
      opacity: 0;

    }

    .showBtn {
      opacity: 1;
    }

    /* Rounded border */
    hr.rounded {
      margin-top: 4%;
      margin-bottom: 3%;
      border-top: 5px solid #bbb;
      border-radius: 3px;
    }

    figure {
      /* display: block; */
      margin-left: auto;
      margin-right: auto;
      text-align: center;
    }

    figcaption {
      background-color: black;
      color: white;
      font-style: italic;
      padding: 2px;
      text-align: center;
    }

    .th1,
    .td1 {
      border: 1px solid black;
      border-radius: 10px;
      padding: 5px 25px;
    }
  </style>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <title>RAG chatbot powered by 🔗 Langchain, OpenAI, Google Generative AI and Hugging Face APIs</title>
  <meta name="viewport" content="width=device-width,minimum-scale=1">
  <meta name="description" content="Image classification in PyTorch using Convolutional and Transformer models">
  <meta name="generator" content="Hugo 0.118.2">




  <meta name="robots" content="noindex, nofollow">



  <link rel="stylesheet" href="https://alagrine.github.io/ananke/css/main.min.css">







  <link rel="shortcut icon" href="https://alagrine.github.io/images/portfolio.png" type="image/x-icon" />






  <meta property="og:title" content="Artwork Classification in PyTorch" />
  <meta property="og:description"
    content="Image classification in PyTorch using Convolutional and Transformer models" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="https://alagrine.github.io/post/p9-rag-chtbot-with-langchain/" />
  <meta property="article:section" content="post" />
  <meta property="article:published_time" content="2024-02-13T01:23:48-04:00" />
  <meta property="article:modified_time" content="2024-02-13T01:23:48-04:00" />
  <meta itemprop="name" content="Artwork Classification in PyTorch">
  <meta itemprop="description" content="Image classification in PyTorch using Convolutional and Transformer models">
  <meta itemprop="datePublished" content="2024-02-13T01:23:48-04:00" />
  <meta itemprop="dateModified" content="2024-02-13T01:23:48-04:00" />
  <meta itemprop="wordCount" content="2594">
  <meta itemprop="keywords" content="" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Artwork Classification in PyTorch" />
  <meta name="twitter:description"
    content="Image classification in PyTorch using Convolutional and Transformer models" />


</head>

<body class="ma0 avenir bg-near-white">






  <header class="cover bg-top" style="background-image: url('https://alagrine.github.io/images/P9/fig1.png');">
    <div class="bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
        <div class="flex-l justify-between items-center center">
          <a href="https://alagrine.github.io/" class="f3 fw2 hover-white no-underline white-90 dib">

            Ala Eddine GRINE

          </a>
          <div class="flex-l items-center">



            <ul class="pl0 mr3">

              <li class="list f5 f4-ns fw4 dib pr3">
                <a class="hover-white no-underline white-90" href="https://alagrine.github.io/about/"
                  title="About page">
                  About
                </a>
              </li>

              <li class="list f5 f4-ns fw4 dib pr3">
                <a class="hover-white no-underline white-90" href="https://alagrine.github.io/post/"
                  title="Projects page">
                  Projects
                </a>
              </li>

            </ul>


            <div class="ananke-socials">


              <a href="https://github.com/AlaGrine" target="_blank" rel="noopener"
                class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
                title="GitHub link" aria-label="follow on GitHub——Opens in a new window">

                <span class="icon"><svg style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512"
                    xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z" />
                  </svg>
                </span>

                <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                    viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                    xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                      style="fill-rule:evenodd;clip-rule:evenodd;" />
                  </svg>
                </span></a>


              <a href="https://www.linkedin.com/in/ala-eddine-grine" target="_blank" rel="noopener"
                class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
                title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window">

                <span class="icon"><svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"
                    xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z"
                      style="fill-rule:evenodd;clip-rule:evenodd;" />
                  </svg>
                </span>

                <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                    viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                    xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                      style="fill-rule:evenodd;clip-rule:evenodd;" />
                  </svg>
                </span></a>


              <a href="https://medium.com/@alaeddine.grine" target="_blank" rel="noopener"
                class="medium ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
                title="Medium link" aria-label="follow on Medium——Opens in a new window">

                <span class="icon"><svg style="enable-background:new 0 0 170 170;" version="1.1" viewBox="0 0 170 170"
                    xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M46.5340803,65.2157554 C46.6968378,63.6076572 46.0836,62.018231 44.8828198,60.93592 L32.6512605,46.2010582 L32.6512605,44 L70.6302521,44 L99.9859944,108.380952 L125.794585,44 L162,44 L162,46.2010582 L151.542017,56.2281011 C150.640424,56.9153477 150.193188,58.0448862 150.380019,59.1628454 L150.380019,132.837155 C150.193188,133.955114 150.640424,135.084652 151.542017,135.771899 L161.755369,145.798942 L161.755369,148 L110.38282,148 L110.38282,145.798942 L120.963119,135.527337 C122.002801,134.487948 122.002801,134.182246 122.002801,132.592593 L122.002801,73.0417402 L92.585901,147.755438 L88.6106443,147.755438 L54.3622782,73.0417402 L54.3622782,123.115814 C54.0767278,125.221069 54.7759199,127.3406 56.2581699,128.863022 L70.0186741,145.55438 L70.0186741,147.755438 L31,147.755438 L31,145.55438 L44.7605042,128.863022 C46.2319621,127.338076 46.8903838,125.204485 46.5340803,123.115814 L46.5340803,65.2157554 Z"
                      style="fill-rule:evenodd;clip-rule:evenodd;" />
                  </svg>
                </span>

                <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                    viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                    xmlns:xlink="http://www.w3.org/1999/xlink">
                    <path
                      d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                      style="fill-rule:evenodd;clip-rule:evenodd;" />
                  </svg>
                </span></a>

            </div>

          </div>
        </div>
      </nav>

      <div class="tc-l pv6 ph3 ph4-ns">

        <div class="f2 f1-l fw2 white-90 mb0 lh-title">Artwork Classification in PyTorch</div>

        <div class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
          Image classification in PyTorch using Convolutional and Transformer models
        </div>


      </div>
    </div>
  </header>



  <main class="pb7" role="main">


    <article class="flex-l flex-wrap justify-between mw8 center ph3">
      <header class="mt4 w-100">
        <aside class="instapaper_ignoref b helvetica tracked">

          PROJECTS
        </aside>











        <div id="sharing" class="mt3 ananke-socials">


          <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://alagrine.github.io/post/p9-rag-chtbot-with-langchain/&amp;title=Artwork%20Classification%20in%20PyTorch"
            class="ananke-social-link linkedin no-underline" aria-label="share on LinkedIn">

            <span class="icon"> <svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"
                xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span>

          </a>

        </div>


        <h1 class="f1 athelas mt3 mb1">Artwork Classification in PyTorch</h1>



        <time class="f6 mv4 dib tracked" datetime="2024-02-13T01:23:48-04:00">February 13, 2024</time>




      </header>
      <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-100">
        <h1 id="project-overview">Project Overview:</h1>
        <p>Although Large Language Models (LLMs) are powerful and capable of generating creative content, they can
          produce outdated or incorrect information as they are trained on static data. To overcome this limitation,
          Retrieval Augmented Generation (RAG) systems can be used to connect the LLM to external data and obtain more
          reliable answers.</p>
        <p>Here is an example: In their recent paper, the authors introduce “Diffuse to Choose”, a novel
          image-conditioned inpainting model based on diffusion.</p>
        <p>I asked ChatGPT what “Diffuse to Choose” means. Here is the answer:</p>
        <blockquote>
          <p><em>“Diffuse to Choose” is not a commonly used phrase or term. Without further context, it is difficult to
              determine its exact meaning.</em></p>
        </blockquote>
        <p>The aim of this project is to build a RAG chatbot in <a
            href="https://python.langchain.com/docs/get_started/introduction">Langchain</a> powered by <a
            href="https://platform.openai.com/overview">OpenAI</a>, <a href="https://ai.google.dev/?hl=en">Google
            Generative AI</a> and <a href="https://huggingface.co/">Hugging Face</a> <strong>APIs</strong>. You can can
          upload documents in txt, pdf, CSV, or docx formats and chat with your data. Relevant documents will be
          retrieved and sent to the LLM along with your follow-up questions for accurate answers.</p>
        <p>This article delves into each component of the RAG system, from the document loader to the conversational
          retrieval chain. Additionally, a user interface is developed using the <a
            href="https://streamlit.io/">streamlit</a> application.</p>
        <div class="mb-4">
          <hr class="rounded">
        </div>
        <h1 id="conversational-rag-architecture">Conversational RAG Architecture</h1>
        <p>Here is an illustration of the architecture and the workflow of the RAG chatbot that we will be building
          using Langchain.</p>
        <figure><img src="https://alagrine.github.io/images/P9/RAG_architecture.png" />
        </figure>

        <p>There are two main blocks in the RAG architecture:</p>
        <ul>
          <li>
            <p>The first block includes a document loader, text splitter, vector store, and retriever. It loads external
              documents, converts them into numerical representations (embeddings), and stores them in a vector store,
              such as a Chroma vector database.</p>
          </li>
          <li>
            <p>The second block comprises LLMs, memory, and prompt templates. It interfaces with the retriever to
              retrieve documents similar to the query, augments the LLM prompt with these documents, and communicates
              with the LLM to obtain an accurate answer.</p>
          </li>
        </ul>
        <p>Here are the main steps in the RAG workflow:</p>
        <ul>
          <li>
            <p><strong>(1), (2), (3) and (4):</strong> The standalone question prompt is formatted with the follow-up
              question and the chat history and passed to the LLM, which rephrases the follow-up question to get a
              standalone question.</p>
            <p>For example, suppose the chat history consists of the user’s question “What does DTC stand for?” and the
              AI’s answer “DTC stands for Diffuse to Choose”. The follow-up question is “Please provide more details
              about it, including its use cases and implementation”. The LLM rephrases this question and replaces “it”
              with “DTC” to obtain the following standalone question: “What are the use cases and implementation of
              Diffuse to Choose (DTC)?”</p>
          </li>
          <li>
            <p><strong>(5), (6), (7) and (8):</strong> The next step is to search for relevant information. The
              retriever compares the embeddings of the standalone question with the Chroma vectorstore. Relevant
              documents are retrieved.</p>
          </li>
          <li>
            <p><strong>(9) and (10):</strong> The LLM prompt is augmented with retrieved documents (and chat history)
              and passed to the LLM in order to obtain a reliable answer.</p>
          </li>
          <li>
            <p><strong>(11):</strong> The memory is updated with the follow-up question and the AI’s answer.</p>
          </li>
        </ul>
        <p>We will dive deeper into each component and each step in the following sections.</p>
        <div class="mb-4">
          <hr class="rounded">
        </div>
        <h1 id="conversational-rag-implementation">Conversational RAG Implementation</h1>
        <p>Let’s start with Retrieval. It includes document loaders, text splitting into chunks, vector stores and
          embeddings, and finally, retrievers.</p>
        <br>
        <h2 id="document-loaders">Document loaders</h2>
        <p>LangChain offers more than 80 <a
            href="https://python.langchain.com/docs/integrations/document_loaders">document loaders</a> to simplify the
          process of loading data from various sources. These sources include the web, cloud services like AWS S3, local
          files (such as CSV and JSON), git, emails, and more.</p>
        <p>To retrieve files from a temporary directory (TMP_DIR) for our application, we will use the
          <strong>DirectoryLoader</strong>. This loader can handle files in txt, pdf, CSV, or docx format. We can filter
          out these formats using the <strong>glob</strong> parameter. The <strong>loader_cls</strong> parameter defines
          the loader class for each format. For example, <strong>TextLoader</strong> is used for txt files.
        </p>
        <p>Below is a code snippet for loading documents.</p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/332aa31d72b681545d77b586ca1ac5b0.js"></script>

        <br>
        <h2 id="text-splitters">Text Splitters</h2>
        <p>The text splitter divides documents into smaller sections that fit within the model’s context window. In
          Langchain, we can split by token, character, or even split code such as Java, JavaScript, and PHP.</p>
        <p>For generic text it is recommended to use the <a
            href="https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter">RecursiveCharacterTextSplitter</a>,
          as it preserves the semantic relationship between paragraphs, sentences and words by keeping them together as
          much as possible.</p>
        <p>To ensure consistency, we use a small overlap between two chunks. This allows for the same context to be
          found at the end of one chunk and the start of the other.</p>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code>text_splitter = <span style="color:cornflowerblue;">RecursiveCharacterTextSplitter</span>(
    separators = [&#34;\n\n&#34;, &#34;\n&#34;, &#34; &#34;, &#34;&#34;],
    chunk_size = 1600,
    chunk_overlap= 200
)

<span style="color:coral;"># Text splitting</span>
chunks = text_splitter.<span style="color:cornflowerblue;">split_documents</span>(documents=documents)
</code></pre>
        <br>
        <h2 id="vectorsores-and-embeddings">Vectorsores and Embeddings</h2>
        <p>Now that the documents are divided into small, meaningful chunks, we can retrieve the chunks that are most
          similar to the query. We will first generate embeddings for these chunks and then store them in a vectorstore.
        </p>
        <h3 id="text-embeddings">Text embeddings</h3>
        <p>Embeddings are numerical representations of text data in a high-dimensional vector space. For instance, the
          size of the embeddings vector size for OpenAI’s <em>text-embedding-ada-002</em> model is 1536.</p>
        <p>To identify the most similar documents to a query, we can search for vectors with the highest similarity to
          the query’s embeddings. Cosine similarity is commonly used to measure the similarity between two vectors.</p>
        <p><a href="https://python.langchain.com/docs/integrations/text_embedding/openai">OpenAI</a>, <a
            href="https://python.langchain.com/docs/integrations/text_embedding/google_generative_ai">Google Generative
            AI</a> and <a href="https://python.langchain.com/docs/integrations/text_embedding/huggingfacehub">Huggin
            Face</a> offer distinct embedding models. The following models will be used:</p>
        <table>
          <thead>
            <tr>
              <th class="td1">Provider</th>
              <th class="td1">Model</th>
              <th class="td1 text-center">Vector dimension</th>
              <th class="td1">Price</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="td1">OpenAI</td>
              <td class="td1"><a
                  href="https://platform.openai.com/docs/guides/embeddings/embedding-models">text-embedding-ada-002</a>
              </td>
              <td class="td1 text-center">1536</td>
              <td class="td1"><strong>$0.00010 / 1K tokens.</strong></td>
            </tr>
            <tr>
              <td class="td1">Google</td>
              <td class="td1"><a href="https://ai.google.dev/models/gemini?hl=en">models/embedding-001</a></td>
              <td class="td1 text-center">768</td>
              <td class="td1"><strong>Rate limit:</strong> 1500 requests per minute.</td>
            </tr>
            <tr>
              <td class="td1">Hugging Face</td>
              <td class="td1"><a href="https://huggingface.co/thenlper/gte-large">thenlper/gte-large</a></td>
              <td class="td1 text-center">1024</td>
              <td class="td1"><strong>free</strong></td>
            </tr>
          </tbody>
        </table>
        <br>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code>def <span style="color:cornflowerblue;">select_embeddings_model</span>(LLM_service=&#34;OpenAI&#34;):
    <span style="color:coral;">&#34;&#34;&#34;Connect to the embeddings API endpoint by specifying
    the name of the embedding model.&#34;&#34;&#34;</span>
    if LLM_service == &#34;OpenAI&#34;:
        embeddings = <span style="color:cornflowerblue;">OpenAIEmbeddings</span>(
            model=<span style="color:coral;">&#39;text-embedding-ada-002&#39;</span>,
            api_key=openai_api_key)

    if LLM_service == &#34;Google&#34;:
        embeddings = <span style="color:cornflowerblue;">GoogleGenerativeAIEmbeddings</span>(
            model=<span style="color:coral;">&#34;models/embedding-001&#34;</span>,
            google_api_key=google_api_key
        )
    if LLM_service == &#34;HuggingFace&#34;:
        embeddings = <span style="color:cornflowerblue;">HuggingFaceInferenceAPIEmbeddings</span>(
            api_key=HF_key,
            model_name=<span style="color:coral;">&#34;thenlper/gte-large&#34;</span>
        )

    return embeddings
</code></pre>
        <h3 id="vectorstores">Vectorstores</h3>
        <p>A vectorstore is a database used to store embedding vectors. It allows for searching vectors that are most
          similar to the query’s embeddings.</p>
        <p>There are several open-source options for vector storage. We will use the <a
            href="https://python.langchain.com/docs/integrations/vectorstores/chroma">Chroma</a> vector database.</p>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code>def <span style="color:cornflowerblue;">create_vectorstore</span>(embeddings,documents,vectorstore_name):
    <span style="color:coral;">&#34;&#34;&#34;Create a Chroma vector database.&#34;&#34;&#34;</span>
    persist_directory = (LOCAL_VECTOR_STORE_DIR.as_posix() + &#34;/&#34; + vectorstore_name)
    vector_store = <span style="color:cornflowerblue;">Chroma.from_documents</span>(
        documents=documents,
        embedding=embeddings,
        persist_directory=persist_directory
    )
    return vector_store
</code></pre>
        <p>Relevant documents are retrieved from the vectorstore by comparing the query’s embeddings to all the vectors
          in the vectorstore using <strong>cosine similarity</strong>.</p>
        <br>
        <h2 id="retrievers">Retrievers</h2>
        <p>A <a href="https://python.langchain.com/docs/modules/data_connection/retrievers/">retriever</a> is
          responsible for returning relevant documents to a query.</p>
        <p>We will start with a simple retriever: the <strong>Vectorstore-backed retriever</strong>. It uses semantic
          search to retrieve documents from a Vectorstore.</p>
        <h3 id="vectorstore-backed-retriever">Vectorstore-backed retriever</h3>
        <p>It uses semantic search to retrieve documents from a Vectorstore. It can perform three types of search:</p>
        <ul>
          <li><strong><em>Similarity search:</em></strong> it returns the k most similar vectors.</li>
          <li><strong><em>Maximum marginal relevance search (MMR):</em></strong> it’s used to ensure both similarity to
            the query and diversity of the selected documents.</li>
          <li><strong><em>Similarity_score_threshold:</em></strong> defines the minimum relevance threshold.</li>
        </ul>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/26f61fe9cd4410169b3d93599f53fc01.js"></script>

        <blockquote>
          <p><strong>When using a Vectorstore-backed retriever to retrieve documents, irrelevant information that is not
              related to the query context is often included. Removing this irrelevant information can lead to more
              cost-effective and accurate LLM calls.</strong></p>
        </blockquote>
        <h3 id="contextual-compression-retriever">Contextual Compression Retriever</h3>
        <p>The Contextual Compression Retriever removes irrelevant information from retrieved documents.</p>
        <p>It first passes the query to the <strong><em>base retriever</em></strong> (vectorstore-backed retriever),
          which returns initial documents. The initial documents are then passed through a <strong><em>Document
              Compressor</em></strong>, which reduces or eliminates them.</p>
        <p>The document compressor can make an <a
            href="https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression#adding-contextual-compression-with-an-llmchainextractor">LLM
            call</a> to perform contextual compression on each retrieved document. This process can be slow and costly.
        </p>
        <p>Instead, we will create a <a
            href="https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression#stringing-compressors-and-document-transformers-together">Document
            Compressor Pipeline</a> as follows:</p>
        <ol>
          <li>Transform the initial retrieved documents by splitting them into smaller chunks using
            <strong><em>CharacterTextSplitter</em></strong> with a chunk size of 500.
          </li>
          <li>Filter out redundant chunks using <code>EmbeddingsRedundantFilter</code>.</li>
          <li>Use <code>EmbeddingsFilter</code> to filter out the most relevant chunks to the query using
            similarity_threshold and K parameters. We will set k to 16.</li>
          <li>Reorder the chunks documents <code>LongContextReorder</code>, so that more relevant elements will be at
            the top and bottom of the list. This will improve LLM performance, as explained <a
              href="https://arxiv.org/abs/2307.03172">here</a>.</li>
        </ol>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/930ad7da30bae36355f2a85bf3912699.js"></script>

        <h3 id="cohere-reranker">Cohere Reranker</h3>
        <p>We will use the <a href="https://docs.cohere.com/docs/reranking">Cohere rerank endpoint</a> to re-order the
          results based on their semantic relevance to the query.
          We will wrap our base retriever with a ContextualCompressionRetriever. The compressor here is the Cohere
          Reranker.
          <script type="application/javascript"
            src="https://gist.github.com/AlaGrine/539d32745ad53c51067ec9365b729ca3.js"></script>
        </p>
        <div class="mb-4">
          <hr class="rounded">
        </div>
        <h1 id="conversational-retrieval-chain-with-memory">Conversational retrieval Chain with memory</h1>
        <p>After retrieving the most relevant documents, the next step is to add them to the LLM prompt, which will be
          sent to the LLM.</p>
        <p>The main components to be used here are: <strong><em>ChatModel</em></strong>,
          <strong><em>PromptTemplate</em></strong>, <strong><em>Memory</em></strong> and
          <strong><em>ConversationalRetrievalChain</em></strong>.
        </p>
        <br>
        <h2 id="chatmodel">ChatModel</h2>
        <p>The ChatModel interacts with LLMs, such as <em>GPT3.5-turbo</em> and <em>Google gemini-pro</em>.</p>
        <p>We will use the OpenAI, Google and Hugging Face APIs, and leverage the ChatOpenAI, ChatGoogleGenerativeAI and
          HuggingFaceHub Langchain classes to instantiate the following pre-trained models:</p>
        <table class="center">
          <thead>
            <tr>
              <th class="td1">Provider</th>
              <th class="td1">Model</th>
              <th class="td1 text-center">maximum token limit</th>
              <th class="td1">Price</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="td1">OpenAI</td>
              <td class="td1"><a href="https://platform.openai.com/docs/models/gpt-3-5-turbo">gpt-3.5-turbo</a></td>
              <td class="td1 text-center">4096</td>
              <td class="td1">Updated pricing can be found on the <a href="https://openai.com/pricing">OpenAI pricing
                  page</a></td>
            </tr>
            <tr>
              <td class="td1">OpenAI</td>
              <td class="td1"><a href="https://platform.openai.com/docs/models/gpt-3-5-turbo">gpt-3.5-turbo-0125</a>
              </td>
              <td class="td1 text-center">16K</td>
              <td class="td1">Updated pricing can be found on the <a href="https://openai.com/pricing">OpenAI pricing
                  page</a></td>
            </tr>
            <tr>
              <td class="td1">OpenAI</td>
              <td class="td1"><a
                  href="https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo">gpt-4-turbo-preview</a></td>
              <td class="td1 text-center">128K</td>
              <td class="td1">Updated pricing can be found on the <a href="https://openai.com/pricing">OpenAI pricing
                  page</a></td>
            </tr>
            <tr>
              <td class="td1">Google</td>
              <td class="td1"><a href="https://ai.google.dev/models/gemini?hl=en">gemini-pro</a></td>
              <td class="td1 text-center">32760</td>
              <td class="td1">Free access to Gemini pro through Google AI Studio with up to <strong>60</strong> requests
                per minute.
              </td>
            </tr>
            <tr>
              <td class="td1">Hugging Face</td>
              <td class="td1"><a
                  href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2">Mistral-7B-Instruct-v0.2</a></td>
              <td class="td1"></td>
              <td class="td1">Free access through 🤗 Hub</td>
            </tr>
          </tbody>
        </table>
        <p>While GPT-4 has 1.8 trillion parameters, <a
            href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2">Mistral-7B-Instruct-v0.2</a>, an
          open-source model from Hugging Face, has 7 billion parameters. Despite its smaller size, Mistral still
          delivers strong performance compared to larger models.</p>
        <p>The chat model can be instantiated as follows:</p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/b60c25c7c9596558772d3cb4c31f53c1.js"></script>

        <p>The main parameters that we can adjust are:</p>
        <ul>
          <li><strong><em>temperature:</em></strong> controls the degree of randomness in token selection. Higher values
            increase diversity, and hence, creativity.</li>
          <li><strong><em>top_k:</em></strong> Selects the next token from the most probable <strong>k</strong> tokens
            by using temperature. Lower k focuses on more probable tokens.</li>
          <li><strong><em>top_p:</em></strong> The cumulative probability cutoff for token selection. Higher values
            increase diversity.</li>
        </ul>
        <p>We alse need four API keys:</p>
        <ul>
          <li><strong>OpenAI</strong> API key: <a href="https://platform.openai.com/account/api-keys">Get an API key</a>
          </li>
          <li><strong>Google</strong> API key: <a href="https://makersuite.google.com/app/apikey">Get an API key</a>
          </li>
          <li><strong>🤗 Hugging Face Hub</strong> API token: <a href="https://huggingface.co/settings/tokens">Get an
              Access Token</a></li>
          <li><strong>Cohere</strong> API key: <a href="https://dashboard.cohere.com/api-keys">Get an API key</a></li>
        </ul>
        <p>First, we need to add the environment variables: <em>OPENAI_API_KEY</em>, <em>GOOGLE_API_KEY</em>,
          <em>HUGGINGFACEHUB_API_TOKEN</em> and <em>COHERE_API_KEY</em>.
        </p>
        <p>Next, we load them as follows.</p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/2e7109e434a962897554b1cd13c701ad.js"></script>

        <br>
        <h2 id="memory">Memory</h2>
        <p>The memory enables the storage of chat history. This can range from a simple buffer, to storing the last K
          interactions or tokens, or summarising the conversation or part of it.</p>
        <p>By default, we will use a simple <strong><em>ConversationBufferMemory</em></strong>.</p>
        <p>For LLMs with a small maximum number of tokens, such as gpt-3–5-turbo, we will use a conversation summary
          buffer. This buffer keeps track of recent interactions based on tokens and summarizes older messages.
          <script type="application/javascript"
            src="https://gist.github.com/AlaGrine/52c81125e2823bb202eb348e423ed660.js"></script>
        </p>
        <ul>
          <li>The <strong><em>return_messages</em></strong> is set to True so that a list of chat messages is returned
            (by default, these messages are concatenated and returned as a single string).</li>
          <li>The <strong><em>output_key</em></strong> is set to &lsquo;answer&rsquo;, and the
            <strong><em>input_key</em></strong> is set to &lsquo;question&rsquo;. This enables the memory to keep track
            of and save user questions and AI answers.
          </li>
        </ul>
        <br>
        <h2 id="prompt-templates">Prompt templates</h2>
        <p>Prompt templates generate prompts for the LLM. For our RAG chatbot, we need two templates.</p>
        <p>The first template asks the LLM to generate a standalone question given the chat history and a follow-up
          question. The <strong><em>PromptTemplate</em></strong> uses this template to return a string prompt when
          invoked.</p>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code>standalone_question_template = <span style="color:coral;">&#34;&#34;&#34;Given the following conversation and a follow up question,
rephrase the follow up question to be a standalone question, in its original language.\n\n
Chat History:\n</span><span style="color:cornflowerblue;">{chat_history}</span><span style="color:coral;">\n
Follow Up Input:</span> <span style="color:cornflowerblue;">{question}</span><span style="color:coral;">\n
Standalone question:&#34;&#34;&#34;</span>

standalone_question_prompt = <span style="color:cornflowerblue;">PromptTemplate</span>(
    input_variables=[&#39;chat_history&#39;, &#39;question&#39;],
    template=standalone_question_template
)
</code></pre>
        <p>The second template includes a placeholder for the context (i.e. retrieved documents), chat history, and the
          user’s question. It instructs the LLM to answer the question based solely on the provided context. The
          <strong><em>ChatPromptTemplate</em></strong> uses this template to return a list of chat messages.
        </p>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code>def <span style="color:cornflowerblue;">answer_template</span>(language=&#34;english&#34;):
    <span style="color:coral;">&#34;&#34;&#34;Pass the standalone question along with the chat history and context
    to the `LLM` wihch will answer&#34;&#34;&#34;</span>

    template = f<span style="color:coral;">&#34;&#34;&#34;Answer the question at the end, using only the following context (delimited by &lt;context&gt;&lt;/context&gt;).
Your answer must be in the language at the end.

&lt;context&gt;</span>
<span style="color:cornflowerblue;">{{chat_history}}</span>

<span style="color:cornflowerblue;">{{context}}</span>
<span style="color:coral;">&lt;/context&gt;

Question: </span><span style="color:cornflowerblue;">{{question}}</span>

<span style="color:coral;">Language:</span><span style="color:cornflowerblue;">{language}</span>
<span style="color:coral;">.&#34;&#34;&#34;</span>
    return template
</code></pre>
        <br>
        <h2 id="conversationalretrievalchain">ConversationalRetrievalChain</h2>
        <p>All the components we have built so far, including the Retriever, ChatModel or LLM, Memory and Prompts, come
          in handy here as they are passed as parameters to the <a
            href="https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html#langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain">ConversationalRetrievalChain</a>.
        </p>
        <p>The <em>ConversationalRetrievalChain</em> is a built-in chain that &ldquo;chains&rdquo; our components
          together, allowing us to chat with our documents. First, it passes the follow-up question and chat history to
          the LLM, which rephrases the question and creates a standalone query. The Retriever then retrieves relevant
          documents (context) based on this query. These documents, along with the standalone question and chat history,
          are then passed to the LLM for answering.</p>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code><span style="color:coral;"># Here is an example using &#34;Mistral-7B-Instruct-v0.2&#34; model from Hugging Face</span>

chain = <span style="color:cornflowerblue;">ConversationalRetrievalChain.from_llm</span>(
    condense_question_prompt=standalone_question_prompt,
    combine_docs_chain_kwargs={&#39;prompt&#39;: answer_prompt},
    condense_question_llm=instantiate_LLM(
        LLM_provider=<span style="color:coral;">&#34;HuggingFace&#34;</span>,api_key=HF_key,temperature=<span style="color:cornflowerblue;">0.1</span>,
        model_name=<span style="color:coral;">&#34;mistralai/Mistral-7B-Instruct-v0.2&#34;</span>),

    memory=create_memory(<span style="color:coral;">&#34;Mistral-7B-Instruct-v0.2&#34;</span>),
    retriever = base_retriever_HF,
    llm=instantiate_LLM(
        LLM_provider=<span style="color:coral;">&#34;HuggingFace&#34;</span>,api_key=HF_key,temperature=<span style="color:cornflowerblue;">0.5</span>,
        model_name=<span style="color:coral;">&#34;mistralai/Mistral-7B-Instruct-v0.2&#34;</span>),
    chain_type= <span style="color:coral;">&#34;stuff&#34;</span>,
    verbose= False,
    return_source_documents=<span style="color:cornflowerblue;">True</span>
)
</code></pre>
        <div class="mb-4">
          <hr class="rounded">
        </div>
        <h1 id="a-step-by-step-approach-to-the-conversational-retrieval-chain">A step-by-step approach to the
          Conversational Retrieval Chain</h1>
        <p>In the previous section, we leveraged the built-in <em>ConversationalRetrievalChain</em> to create our RAG
          model.</p>
        <p>This section will use a step-by-step approach to help us understand what’s happening under the hood and
          customize the RAG chain.
          For instance, we can return the standalone question and format retrieved documents.</p>
        <br>
        <h2 id="first-step-create-a-standalone_question-chain">First step: Create a standalone_question chain</h2>
        <p>Here, we load the chat history and pass it along with the follow-up question to the LLM. The LLM combines
          them and generates a standalone question (new query).</p>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/fae46a98ede5985712a2f318302440f7.js"></script>

        <p>This is an example of how to invoke chain_question:</p>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code>memory.<span style="color:cornflowerblue;">clear()</span>
memory.<span style="color:cornflowerblue;">save_context</span>(
    {&#34;question&#34;: <span style="color:coral;">What does DTC stand for?&#34;</span>&#34;},
    {&#34;answer&#34;: <span style="color:coral;">&#34;Diffuse to Choose.&#34;</span>}
)

print(<span style="color:coral;">&#34;Chat history:\n&#34;</span>,memory.<span style="color:cornflowerblue;">load_memory_variables({})</span>)
follow_up_question = <span style="color:coral;">&#34;plaese give more details about it, including its use cases and implementation.&#34;</span>
print(<span style="color:coral;">\nFollow-up question:\n</span>,follow_up_question)

<span style="color:coral;"># invoke chain_question</span>
response = chain_question.<span style="color:cornflowerblue;">invoke</span>({&#34;question&#34;:follow_up_question})[&#39;standalone_question&#39;]
print(&#34;\nStandalone_question:\n&#34;,response)
</code></pre>
        <p>Here is the response:</p>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code><span style="color:coral;">Chat history:</span>
 {&#39;chat_history&#39;:
   [HumanMessage(content=&#39;What does DTC stand for?&#39;),
    AIMessage(content=&#39;Diffuse to Choose.&#39;)
   ]
}

<span style="color:coral;">Follow-up question:</span>
plaese give more details about <span style="color:cornflowerblue;">it</span>, including <span style="color:cornflowerblue;">its</span> use cases and implementation.

<span style="color:coral;">Standalone_question:</span>
 What are the use cases and implementation of <span style="color:cornflowerblue;">Diffuse to Choose (DTC)</span>?
</code></pre>
        <p>We can see that “it” has been replaced by “Diffuse to Choose (DTC)”, to which it refers.</p>
        <br>
        <h2
          id="seconde-step-retrieve-documents-pass-them-to-the-llm-along-with-the-standalone-question-and-chat-history-and-parse-the-response">
          Seconde step: Retrieve documents, pass them to the LLM along with the standalone question and chat history,
          and parse the response.</h2>
        <script type="application/javascript"
          src="https://gist.github.com/AlaGrine/3b4855c9f68ae1cdd3eeb0ee58bcaa39.js"></script>

        <p>To create the conversational retrieval chain, let’s chain the two chains: chain_question and chain_answer.
        </p>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code>conversational_retriever_chain = chain_question <span style="color:cornflowerblue;">|</span> retrieved_documents <span style="color:cornflowerblue;">|</span> chain_answer
</code></pre>
        <p>Let’s invoke this chain:</p>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code>follow_up_question = <span style="color:coral;">&#34;plaese give more details about it, including its use cases and implementation.&#34;</span>

response = conversational_retriever_chain.<span style="color:cornflowerblue;">invoke</span>({&#34;question&#34;:follow_up_question})
Markdown(response[&#39;answer&#39;].<span style="color:cornflowerblue;">content</span>)
</code></pre>
        <p>Here is the AI response:</p>
        <blockquote>
          <p><em>Diffuse to Choose (DTC) is a novel diffusion-based image-conditioned inpainting model that efficiently
              balances fast inference with the retention of high-fidelity details in a given reference item while
              ensuring accurate semantic manipulations in the given scene content. It is used for Virtual Try-All
              (Vit-All), which allows users to virtually visualize products in their settings…</em></p>
        </blockquote>
        <p>If we check the memory, we will see that it does not update automatically. We need to manually save the
          follow-up question and the AI response as follows:</p>
        <pre tabindex="0" style="overflow-y: auto;overflow-x: auto;"><code>memory.<span style="color:cornflowerblue;">save_context</span>(
 {&#34;question&#34;: follow_up_question},
 {&#34;answer&#34;: response[&#39;answer&#39;].<span style="color:cornflowerblue;">content</span>}
)
</code></pre>
        <p>We have created our RAG system successfully.</p>
        <div class="mb-4">
          <hr class="rounded">
        </div>
        <h1 id="streamlit-application">Streamlit application</h1>
        <p>We leveraged our RAG system to build a Streamlit application that enables document chat. You can find the
          application in this <a href="https://github.com/AlaGrine/RAG_chatabot_with_Langchain">Github repository</a>.
        </p>
        <p>Here is a screenshot of the app:</p>
        <figure><img src="https://alagrine.github.io/images/P9/streamlit_app_1.png" />
        </figure>

        <p>In the sidebar, you can select the LLM provider, choose an LLM, adjust its parameters, and insert your API
          keys.</p>
        <p>In the main panel, you can create or load a Chroma vectorstore and display or clear the chat messages.</p>
        <div class="mb-4">
          <hr class="rounded">
        </div>
        <h1 id="conclusion">Conclusion</h1>
        <p>In this article, we covered all the steps of creating a RAG chatbot in Langchain, from loading documents to
          creating a conversational retrieval chain.
          Additionally, we developed a Streamlit application.</p>
        <p>Our RAG chatbot was powered by OpenAI, Google Generative AI and Hugging Face <strong>APIs</strong>. An
          alternative option is to run open-source quantized models <strong><a
              href="https://python.langchain.com/docs/guides/local_llms">locally</a></strong> to protect privacy and
          avoid inference fees. These models can be run with frameworks such as <a
            href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> and <a
            href="https://github.com/ggerganov/llama.cpp">Ollama</a>.</p>

        <br>
        <p><a href="https://github.com/AlaGrine/RAG_chatabot_with_Langchain">Link to GitHub repository</a></p>


        <span class="badge badge-pill badge-success">AI</span>
        <span class="badge badge-pill badge-success">Langchain</span>
        <span class="badge badge-pill badge-success">LLM</span>
        <span class="badge badge-pill badge-success">Streamlit</span>

        <ul class="pa0">

        </ul>
        <div class="mt6 instapaper_ignoref">


        </div>
      </div>

      <aside class="w-30-l mt6-l">




      </aside>

    </article>

  </main>
  <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
    <div class="flex justify-between">
      <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://alagrine.github.io/">
        &copy; Ala Eddine GRINE 2024
      </a>
      <div>
        <div class="ananke-socials">


          <a href="https://github.com/AlaGrine" target="_blank" rel="noopener"
            class="github ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
            title="GitHub link" aria-label="follow on GitHub——Opens in a new window">

            <span class="icon"><svg style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512"
                xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z" />
              </svg>
            </span>

            <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span></a>


          <a href="https://www.linkedin.com/in/ala-eddine-grine" target="_blank" rel="noopener"
            class="linkedin ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
            title="LinkedIn link" aria-label="follow on LinkedIn——Opens in a new window">

            <span class="icon"><svg style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65"
                xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span>

            <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span></a>


          <a href="https://medium.com/@alaeddine.grine" target="_blank" rel="noopener"
            class="medium ananke-social-link link-transition stackoverflow link dib z-999 pt3 pt0-l mr1"
            title="Medium link" aria-label="follow on Medium——Opens in a new window">

            <span class="icon"><svg style="enable-background:new 0 0 170 170;" version="1.1" viewBox="0 0 170 170"
                xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M46.5340803,65.2157554 C46.6968378,63.6076572 46.0836,62.018231 44.8828198,60.93592 L32.6512605,46.2010582 L32.6512605,44 L70.6302521,44 L99.9859944,108.380952 L125.794585,44 L162,44 L162,46.2010582 L151.542017,56.2281011 C150.640424,56.9153477 150.193188,58.0448862 150.380019,59.1628454 L150.380019,132.837155 C150.193188,133.955114 150.640424,135.084652 151.542017,135.771899 L161.755369,145.798942 L161.755369,148 L110.38282,148 L110.38282,145.798942 L120.963119,135.527337 C122.002801,134.487948 122.002801,134.182246 122.002801,132.592593 L122.002801,73.0417402 L92.585901,147.755438 L88.6106443,147.755438 L54.3622782,73.0417402 L54.3622782,123.115814 C54.0767278,125.221069 54.7759199,127.3406 56.2581699,128.863022 L70.0186741,145.55438 L70.0186741,147.755438 L31,147.755438 L31,145.55438 L44.7605042,128.863022 C46.2319621,127.338076 46.8903838,125.204485 46.5340803,123.115814 L46.5340803,65.2157554 Z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span>

            <span class="new-window"><svg height="8px" style="enable-background:new 0 0 1000 1000;" version="1.1"
                viewBox="0 0 1000 1000" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"
                xmlns:xlink="http://www.w3.org/1999/xlink">
                <path
                  d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z"
                  style="fill-rule:evenodd;clip-rule:evenodd;" />
              </svg>
            </span></a>

        </div>
      </div>
    </div>
  </footer>

  <!-- Add ScrollToTop button; only visible when we scroll up -->
  <div id="scrollToTopBtn" class="scrollToTopBtn"> <i class="fas fa-arrow-up"></i> Back to
    top</div>

  <script>

    window.onscroll = function () {
      if (pageYOffset >= 100) {
        document.getElementById('scrollToTopBtn').style.visibility = "visible";
      } else {
        document.getElementById('scrollToTopBtn').style.visibility = "hidden";
      }
    };
    var scrollToTopBtn = document.querySelector(".scrollToTopBtn");
    var rootElement = document.documentElement;

    let lastScrollTop = window.pageYOffset || document.documentElement.scrollTop;

    function handleScroll() {
      var scrollTotal = rootElement.scrollHeight - rootElement.clientHeight;
      const scrollTopPosition = window.pageYOffset || document.documentElement.scrollTop;

      if (scrollTopPosition > lastScrollTop) {
        // Hide button
        scrollToTopBtn.classList.remove("showBtn");
      } else if (scrollTopPosition < lastScrollTop) {
        // Show button
        scrollToTopBtn.classList.add("showBtn");
      }
      lastScrollTop = scrollTopPosition <= 0 ? 0 : scrollTopPosition;
    }

    function scrollToTop() {
      rootElement.scrollTo({
        top: 0,
        behavior: "smooth"
      });
      handleScroll();
    }
    scrollToTopBtn.addEventListener("click", scrollToTop);
    document.addEventListener("scroll", handleScroll);
  </script>

</body>

</html>